{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iit import generate_data, get_iit_distribution_dataset_both\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from model import TorchDeepNeuralClassifier\n",
    "from model_iit import TorchDeepNeuralClassifierIIT\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import vsm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = ['zero', 'one', 'two', 'three', 'four',\n",
    "       'five', 'six', 'seven', 'eight', 'nine']\n",
    "\n",
    "train_test_split = 0.9\n",
    "X_train, y_train, X_test, y_test = generate_data(vals, train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['four', 'nine', 'four'],\n",
       " ['nine', 'nine', 'zero'],\n",
       " ['six', 'one', 'one'],\n",
       " ['seven', 'three', 'four'],\n",
       " ['five', 'nine', 'three']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 81, 12, 49, 60]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['eight', 'six', 'five'],\n",
       " ['six', 'nine', 'two'],\n",
       " ['two', 'one', 'zero'],\n",
       " ['nine', 'five', 'five'],\n",
       " ['two', 'five', 'five']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a feed-forward network using randomized embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 163\n",
    "num_inputs = 3\n",
    "num_layers = 2\n",
    "embed_dim = 5\n",
    "\n",
    "mod = TorchDeepNeuralClassifier(vals, output_size, num_inputs,\n",
    "                            num_layers, embed_dim, None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1000 of 1000; error is 0.10345329344272614"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TorchDeepNeuralClassifier(\n",
       "\tbatch_size=1028,\n",
       "\tmax_iter=1000,\n",
       "\teta=0.001,\n",
       "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tgradient_accumulation_steps=1,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=False,\n",
       "\tn_iter_no_change=10,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05,\n",
       "\thidden_dim=50,\n",
       "\thidden_activation=Tanh(),\n",
       "\tnum_layers=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93         7\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.67      1.00      0.80         2\n",
      "           8       0.67      0.50      0.57         4\n",
      "          10       0.67      0.67      0.67         3\n",
      "          12       0.50      1.00      0.67         2\n",
      "          13       1.00      1.00      1.00         1\n",
      "          14       1.00      1.00      1.00         1\n",
      "          15       0.67      1.00      0.80         2\n",
      "          16       1.00      0.67      0.80         3\n",
      "          18       1.00      1.00      1.00         2\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       1.00      1.00      1.00         2\n",
      "          22       1.00      1.00      1.00         1\n",
      "          24       1.00      1.00      1.00         1\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         2\n",
      "          30       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         1\n",
      "          33       1.00      1.00      1.00         1\n",
      "          35       1.00      1.00      1.00         1\n",
      "          36       1.00      1.00      1.00         3\n",
      "          39       1.00      1.00      1.00         2\n",
      "          40       1.00      1.00      1.00         2\n",
      "          42       1.00      1.00      1.00         3\n",
      "          45       1.00      1.00      1.00         2\n",
      "          48       1.00      1.00      1.00         2\n",
      "          50       1.00      1.00      1.00         2\n",
      "          54       1.00      1.00      1.00         2\n",
      "          55       1.00      1.00      1.00         2\n",
      "          56       1.00      1.00      1.00         2\n",
      "          60       1.00      1.00      1.00         4\n",
      "          64       1.00      1.00      1.00         1\n",
      "          66       1.00      1.00      1.00         2\n",
      "          70       1.00      1.00      1.00         2\n",
      "          72       1.00      1.00      1.00         1\n",
      "          75       1.00      1.00      1.00         1\n",
      "          77       1.00      1.00      1.00         2\n",
      "          80       1.00      1.00      1.00         2\n",
      "          81       1.00      1.00      1.00         1\n",
      "          84       1.00      1.00      1.00         1\n",
      "          88       1.00      1.00      1.00         2\n",
      "          90       1.00      0.67      0.80         3\n",
      "          91       1.00      1.00      1.00         1\n",
      "          96       0.50      1.00      0.67         1\n",
      "         105       1.00      1.00      1.00         1\n",
      "         108       1.00      0.50      0.67         2\n",
      "         112       1.00      1.00      1.00         1\n",
      "         117       1.00      1.00      1.00         1\n",
      "         128       1.00      1.00      1.00         1\n",
      "         135       1.00      1.00      1.00         1\n",
      "         162       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.86      0.87      0.86       100\n",
      "weighted avg       0.89      0.89      0.88       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = mod.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a feed-forward network using BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_weights_name = 'bert-base-uncased'\n",
    "# Initialize a BERT tokenizer and BERT model based on\n",
    "# `bert_weights_name`:\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "bert_model = BertModel.from_pretrained(bert_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embed = vsm.create_subword_pooling_vsm(\n",
    "    vals, bert_tokenizer, bert_model, layer=1, pool_func=vsm.mean_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>0.368413</td>\n",
       "      <td>0.714821</td>\n",
       "      <td>-0.532399</td>\n",
       "      <td>-0.153238</td>\n",
       "      <td>-0.184203</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.028352</td>\n",
       "      <td>-0.162773</td>\n",
       "      <td>0.163669</td>\n",
       "      <td>-0.471809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637450</td>\n",
       "      <td>-0.782000</td>\n",
       "      <td>0.181008</td>\n",
       "      <td>-1.160265</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>-0.899586</td>\n",
       "      <td>0.350256</td>\n",
       "      <td>-0.099035</td>\n",
       "      <td>-0.274523</td>\n",
       "      <td>0.308869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.213226</td>\n",
       "      <td>0.484864</td>\n",
       "      <td>-0.032716</td>\n",
       "      <td>-0.026842</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>-0.086201</td>\n",
       "      <td>0.195947</td>\n",
       "      <td>-0.169620</td>\n",
       "      <td>0.540456</td>\n",
       "      <td>-0.261407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455036</td>\n",
       "      <td>-0.426076</td>\n",
       "      <td>0.282586</td>\n",
       "      <td>-0.676856</td>\n",
       "      <td>-0.045337</td>\n",
       "      <td>-0.428130</td>\n",
       "      <td>0.227807</td>\n",
       "      <td>0.216206</td>\n",
       "      <td>0.112196</td>\n",
       "      <td>-0.128516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>-0.121083</td>\n",
       "      <td>0.161060</td>\n",
       "      <td>-0.549375</td>\n",
       "      <td>-0.472711</td>\n",
       "      <td>-0.146909</td>\n",
       "      <td>0.155344</td>\n",
       "      <td>-0.133190</td>\n",
       "      <td>-0.483241</td>\n",
       "      <td>0.173656</td>\n",
       "      <td>-0.328344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303860</td>\n",
       "      <td>0.224042</td>\n",
       "      <td>0.113168</td>\n",
       "      <td>-0.807975</td>\n",
       "      <td>-0.281597</td>\n",
       "      <td>-0.575383</td>\n",
       "      <td>0.138091</td>\n",
       "      <td>0.198803</td>\n",
       "      <td>-0.091571</td>\n",
       "      <td>-0.442494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>three</th>\n",
       "      <td>0.104971</td>\n",
       "      <td>0.313852</td>\n",
       "      <td>-0.340210</td>\n",
       "      <td>-0.434407</td>\n",
       "      <td>-0.049186</td>\n",
       "      <td>0.154321</td>\n",
       "      <td>-0.099751</td>\n",
       "      <td>-0.506876</td>\n",
       "      <td>0.389955</td>\n",
       "      <td>-0.244507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379173</td>\n",
       "      <td>0.134788</td>\n",
       "      <td>0.233690</td>\n",
       "      <td>-0.651362</td>\n",
       "      <td>-0.219017</td>\n",
       "      <td>-0.658988</td>\n",
       "      <td>0.203868</td>\n",
       "      <td>0.215337</td>\n",
       "      <td>-0.093137</td>\n",
       "      <td>-0.477617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>four</th>\n",
       "      <td>0.039704</td>\n",
       "      <td>0.210273</td>\n",
       "      <td>-0.526674</td>\n",
       "      <td>-0.221241</td>\n",
       "      <td>-0.102211</td>\n",
       "      <td>0.203096</td>\n",
       "      <td>-0.147871</td>\n",
       "      <td>-0.331833</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>-0.481170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240441</td>\n",
       "      <td>-0.040645</td>\n",
       "      <td>0.122894</td>\n",
       "      <td>-0.991067</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>-0.848153</td>\n",
       "      <td>0.368776</td>\n",
       "      <td>0.249214</td>\n",
       "      <td>-0.265129</td>\n",
       "      <td>-0.095733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>five</th>\n",
       "      <td>-0.039133</td>\n",
       "      <td>0.148628</td>\n",
       "      <td>-0.443105</td>\n",
       "      <td>-0.602812</td>\n",
       "      <td>0.024338</td>\n",
       "      <td>-0.077868</td>\n",
       "      <td>-0.239623</td>\n",
       "      <td>-0.467639</td>\n",
       "      <td>0.117604</td>\n",
       "      <td>-0.661229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410685</td>\n",
       "      <td>-0.126665</td>\n",
       "      <td>0.254384</td>\n",
       "      <td>-0.834398</td>\n",
       "      <td>-0.037767</td>\n",
       "      <td>-0.695847</td>\n",
       "      <td>0.188720</td>\n",
       "      <td>0.237780</td>\n",
       "      <td>-0.017827</td>\n",
       "      <td>-0.366614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>six</th>\n",
       "      <td>0.304316</td>\n",
       "      <td>0.202454</td>\n",
       "      <td>-0.506930</td>\n",
       "      <td>-0.336193</td>\n",
       "      <td>0.042346</td>\n",
       "      <td>-0.201237</td>\n",
       "      <td>-0.326339</td>\n",
       "      <td>-0.235801</td>\n",
       "      <td>0.244673</td>\n",
       "      <td>-0.600611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417288</td>\n",
       "      <td>-0.329488</td>\n",
       "      <td>0.127992</td>\n",
       "      <td>-0.932242</td>\n",
       "      <td>-0.041394</td>\n",
       "      <td>-0.668947</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>0.385106</td>\n",
       "      <td>0.048364</td>\n",
       "      <td>-0.338787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seven</th>\n",
       "      <td>0.092242</td>\n",
       "      <td>0.176112</td>\n",
       "      <td>-0.475537</td>\n",
       "      <td>-0.412275</td>\n",
       "      <td>0.071916</td>\n",
       "      <td>-0.310987</td>\n",
       "      <td>-0.048490</td>\n",
       "      <td>-0.409864</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>-0.807861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095583</td>\n",
       "      <td>-0.178779</td>\n",
       "      <td>0.421195</td>\n",
       "      <td>-0.693458</td>\n",
       "      <td>0.123755</td>\n",
       "      <td>-0.667253</td>\n",
       "      <td>0.076955</td>\n",
       "      <td>-0.138880</td>\n",
       "      <td>-0.079268</td>\n",
       "      <td>-0.244648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eight</th>\n",
       "      <td>-0.033224</td>\n",
       "      <td>0.192466</td>\n",
       "      <td>-0.507554</td>\n",
       "      <td>-0.419275</td>\n",
       "      <td>0.235861</td>\n",
       "      <td>0.105365</td>\n",
       "      <td>-0.255650</td>\n",
       "      <td>-0.200700</td>\n",
       "      <td>-0.053316</td>\n",
       "      <td>-0.733084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>-0.171980</td>\n",
       "      <td>0.286490</td>\n",
       "      <td>-0.620115</td>\n",
       "      <td>-0.104281</td>\n",
       "      <td>-0.584950</td>\n",
       "      <td>0.222393</td>\n",
       "      <td>0.159457</td>\n",
       "      <td>-0.355556</td>\n",
       "      <td>-0.570065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nine</th>\n",
       "      <td>0.105645</td>\n",
       "      <td>0.060501</td>\n",
       "      <td>-0.367164</td>\n",
       "      <td>-0.433360</td>\n",
       "      <td>0.425262</td>\n",
       "      <td>-0.032186</td>\n",
       "      <td>-0.136358</td>\n",
       "      <td>-0.358969</td>\n",
       "      <td>0.129830</td>\n",
       "      <td>-0.574778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081581</td>\n",
       "      <td>-0.280892</td>\n",
       "      <td>0.540993</td>\n",
       "      <td>-0.752147</td>\n",
       "      <td>-0.059858</td>\n",
       "      <td>-0.446215</td>\n",
       "      <td>0.033923</td>\n",
       "      <td>-0.036796</td>\n",
       "      <td>-0.296937</td>\n",
       "      <td>-0.314274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "zero   0.368413  0.714821 -0.532399 -0.153238 -0.184203  0.009934  0.028352   \n",
       "one    0.213226  0.484864 -0.032716 -0.026842  0.090642 -0.086201  0.195947   \n",
       "two   -0.121083  0.161060 -0.549375 -0.472711 -0.146909  0.155344 -0.133190   \n",
       "three  0.104971  0.313852 -0.340210 -0.434407 -0.049186  0.154321 -0.099751   \n",
       "four   0.039704  0.210273 -0.526674 -0.221241 -0.102211  0.203096 -0.147871   \n",
       "five  -0.039133  0.148628 -0.443105 -0.602812  0.024338 -0.077868 -0.239623   \n",
       "six    0.304316  0.202454 -0.506930 -0.336193  0.042346 -0.201237 -0.326339   \n",
       "seven  0.092242  0.176112 -0.475537 -0.412275  0.071916 -0.310987 -0.048490   \n",
       "eight -0.033224  0.192466 -0.507554 -0.419275  0.235861  0.105365 -0.255650   \n",
       "nine   0.105645  0.060501 -0.367164 -0.433360  0.425262 -0.032186 -0.136358   \n",
       "\n",
       "            7         8         9    ...       758       759       760  \\\n",
       "zero  -0.162773  0.163669 -0.471809  ...  0.637450 -0.782000  0.181008   \n",
       "one   -0.169620  0.540456 -0.261407  ...  0.455036 -0.426076  0.282586   \n",
       "two   -0.483241  0.173656 -0.328344  ...  0.303860  0.224042  0.113168   \n",
       "three -0.506876  0.389955 -0.244507  ...  0.379173  0.134788  0.233690   \n",
       "four  -0.331833  0.267574 -0.481170  ...  0.240441 -0.040645  0.122894   \n",
       "five  -0.467639  0.117604 -0.661229  ...  0.410685 -0.126665  0.254384   \n",
       "six   -0.235801  0.244673 -0.600611  ...  0.417288 -0.329488  0.127992   \n",
       "seven -0.409864  0.006404 -0.807861  ...  0.095583 -0.178779  0.421195   \n",
       "eight -0.200700 -0.053316 -0.733084  ...  0.233600 -0.171980  0.286490   \n",
       "nine  -0.358969  0.129830 -0.574778  ...  0.081581 -0.280892  0.540993   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "zero  -1.160265  0.005396 -0.899586  0.350256 -0.099035 -0.274523  0.308869  \n",
       "one   -0.676856 -0.045337 -0.428130  0.227807  0.216206  0.112196 -0.128516  \n",
       "two   -0.807975 -0.281597 -0.575383  0.138091  0.198803 -0.091571 -0.442494  \n",
       "three -0.651362 -0.219017 -0.658988  0.203868  0.215337 -0.093137 -0.477617  \n",
       "four  -0.991067  0.013572 -0.848153  0.368776  0.249214 -0.265129 -0.095733  \n",
       "five  -0.834398 -0.037767 -0.695847  0.188720  0.237780 -0.017827 -0.366614  \n",
       "six   -0.932242 -0.041394 -0.668947 -0.093982  0.385106  0.048364 -0.338787  \n",
       "seven -0.693458  0.123755 -0.667253  0.076955 -0.138880 -0.079268 -0.244648  \n",
       "eight -0.620115 -0.104281 -0.584950  0.222393  0.159457 -0.355556 -0.570065  \n",
       "nine  -0.752147 -0.059858 -0.446215  0.033923 -0.036796 -0.296937 -0.314274  \n",
       "\n",
       "[10 rows x 768 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 163\n",
    "num_inputs = 3\n",
    "num_layers = 2\n",
    "embed_dim = bert_embed.shape[1]\n",
    "freeze_embedding = True\n",
    "\n",
    "mod_bert_embed = TorchDeepNeuralClassifier(vals, output_size, num_inputs,\n",
    "                            num_layers, embed_dim, bert_embed,\n",
    "                            freeze_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1000 of 1000; error is 0.04465832561254501"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TorchDeepNeuralClassifier(\n",
       "\tbatch_size=1028,\n",
       "\tmax_iter=1000,\n",
       "\teta=0.001,\n",
       "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tgradient_accumulation_steps=1,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=False,\n",
       "\tn_iter_no_change=10,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05,\n",
       "\thidden_dim=50,\n",
       "\thidden_activation=Tanh(),\n",
       "\tnum_layers=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_bert_embed.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         7\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.25      1.00      0.40         1\n",
      "           5       1.00      1.00      1.00         1\n",
      "           6       0.33      0.50      0.40         2\n",
      "           8       1.00      0.50      0.67         4\n",
      "          10       1.00      1.00      1.00         3\n",
      "          12       1.00      0.50      0.67         2\n",
      "          13       1.00      1.00      1.00         1\n",
      "          14       0.33      1.00      0.50         1\n",
      "          15       1.00      1.00      1.00         2\n",
      "          16       0.67      0.67      0.67         3\n",
      "          18       1.00      0.50      0.67         2\n",
      "          20       1.00      1.00      1.00         2\n",
      "          21       1.00      1.00      1.00         2\n",
      "          22       1.00      1.00      1.00         1\n",
      "          24       1.00      1.00      1.00         1\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         2\n",
      "          30       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         1\n",
      "          33       1.00      1.00      1.00         1\n",
      "          35       1.00      1.00      1.00         1\n",
      "          36       1.00      1.00      1.00         3\n",
      "          39       1.00      1.00      1.00         2\n",
      "          40       1.00      1.00      1.00         2\n",
      "          42       1.00      1.00      1.00         3\n",
      "          45       1.00      1.00      1.00         2\n",
      "          48       1.00      1.00      1.00         2\n",
      "          50       1.00      1.00      1.00         2\n",
      "          54       1.00      1.00      1.00         2\n",
      "          55       1.00      1.00      1.00         2\n",
      "          56       1.00      1.00      1.00         2\n",
      "          60       1.00      0.75      0.86         4\n",
      "          64       1.00      1.00      1.00         1\n",
      "          66       0.67      1.00      0.80         2\n",
      "          70       1.00      1.00      1.00         2\n",
      "          72       1.00      1.00      1.00         1\n",
      "          75       1.00      1.00      1.00         1\n",
      "          77       1.00      1.00      1.00         2\n",
      "          80       1.00      1.00      1.00         2\n",
      "          81       1.00      1.00      1.00         1\n",
      "          84       1.00      1.00      1.00         1\n",
      "          85       0.00      0.00      0.00         0\n",
      "          88       1.00      1.00      1.00         2\n",
      "          90       1.00      0.67      0.80         3\n",
      "          91       1.00      1.00      1.00         1\n",
      "          96       1.00      1.00      1.00         1\n",
      "         105       1.00      1.00      1.00         1\n",
      "         108       1.00      0.50      0.67         2\n",
      "         112       1.00      1.00      1.00         1\n",
      "         117       1.00      1.00      1.00         1\n",
      "         128       1.00      1.00      1.00         1\n",
      "         135       1.00      1.00      1.00         1\n",
      "         144       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.88       100\n",
      "   macro avg       0.88      0.87      0.86       100\n",
      "weighted avg       0.93      0.88      0.89       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = mod_bert_embed.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a BERT model by tokenizing inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HfBertClassifierModel(nn.Module):\n",
    "    def __init__(self, n_classes, weights_name='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.weights_name = weights_name\n",
    "        self.bert = BertModel.from_pretrained(self.weights_name)\n",
    "        self.bert.train()\n",
    "        self.hidden_dim = self.bert.embeddings.word_embeddings.embedding_dim\n",
    "        # The only new parameters -- the classifier:\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.hidden_dim, self.n_classes)\n",
    "\n",
    "    def forward(self, indices, mask):\n",
    "        reps = self.bert(\n",
    "            indices, attention_mask=mask)\n",
    "        return self.classifier_layer(reps.pooler_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HfBertClassifier(TorchShallowNeuralClassifier):\n",
    "    def __init__(self, weights_name, *args, **kwargs):\n",
    "        self.weights_name = weights_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.weights_name)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.params += ['weights_name']\n",
    "\n",
    "    def build_graph(self):\n",
    "        return HfBertClassifierModel(self.n_classes_, self.weights_name)\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            X,\n",
    "            max_length=None,\n",
    "            add_special_tokens=True,\n",
    "            padding='longest',\n",
    "            return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        if y is None:\n",
    "            dataset = torch.utils.data.TensorDataset(indices, mask)\n",
    "        else:\n",
    "            self.classes_ = sorted(set(y))\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            y = [class2index[label] for label in y]\n",
    "            y = torch.tensor(y)\n",
    "            dataset = torch.utils.data.TensorDataset(indices, mask, y)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion(X):\n",
    "    new_X = []\n",
    "    for inpts in X:\n",
    "        new_X.append(' '.join(inpts))\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_bert = HfBertClassifier('bert-base-uncased', max_iter=5, batch_size=8, n_iter_no_change=5, early_stopping=True, hidden_dim=100, eta=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seven seven four',\n",
       " 'seven nine nine',\n",
       " 'four eight five',\n",
       " 'eight nine two',\n",
       " 'one five zero']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bert = conversion(X_train)\n",
    "X_test_bert = conversion(X_test)\n",
    "X_train_bert[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 250.60060513019562"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HfBertClassifier(\n",
       "\tbatch_size=8,\n",
       "\tmax_iter=5,\n",
       "\teta=5e-05,\n",
       "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tgradient_accumulation_steps=1,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=True,\n",
       "\tn_iter_no_change=5,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05,\n",
       "\thidden_dim=100,\n",
       "\thidden_activation=Tanh(),\n",
       "\tweights_name=bert-base-uncased)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_bert.fit(X_train_bert, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93         8\n",
      "           1       0.00      0.00      0.00         1\n",
      "           4       0.50      0.50      0.50         2\n",
      "           6       0.50      1.00      0.67         1\n",
      "           8       0.00      0.00      0.00         5\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         2\n",
      "          12       0.50      0.20      0.29         5\n",
      "          15       0.00      0.00      0.00         1\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.07      1.00      0.13         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         3\n",
      "          24       0.25      0.60      0.35         5\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.00      0.00      0.00         2\n",
      "          28       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00         6\n",
      "          32       0.00      0.00      0.00         2\n",
      "          36       0.29      1.00      0.44         2\n",
      "          40       0.50      1.00      0.67         3\n",
      "          42       1.00      0.50      0.67         4\n",
      "          44       0.00      0.00      0.00         2\n",
      "          45       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         3\n",
      "          49       0.00      0.00      0.00         0\n",
      "          51       0.00      0.00      0.00         1\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         2\n",
      "          56       0.00      0.00      0.00         3\n",
      "          60       0.00      0.00      0.00         2\n",
      "          63       0.33      1.00      0.50         1\n",
      "          64       0.00      0.00      0.00         1\n",
      "          70       0.00      0.00      0.00         1\n",
      "          72       0.56      0.83      0.67         6\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          81       0.00      0.00      0.00         2\n",
      "          84       0.00      0.00      0.00         2\n",
      "          90       0.20      1.00      0.33         1\n",
      "          91       0.00      0.00      0.00         0\n",
      "          96       1.00      1.00      1.00         1\n",
      "          98       0.00      0.00      0.00         2\n",
      "          99       0.00      0.00      0.00         1\n",
      "         112       0.00      0.00      0.00         3\n",
      "         126       0.00      0.00      0.00         0\n",
      "         135       0.00      0.00      0.00         1\n",
      "         144       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.29       100\n",
      "   macro avg       0.14      0.21      0.15       100\n",
      "weighted avg       0.24      0.29      0.24       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = mod_bert.predict(X_test_bert)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IIT Dataset that computes xy and yz then xy + yz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = 1\n",
    "V2 = 2\n",
    "\n",
    "train_data_V1, test_data_V1 = get_iit_distribution_dataset_both(V1, vals)\n",
    "train_data_V2, test_data_V2 = get_iit_distribution_dataset_both(V2, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_base_train_V1, y_base_train_V1, x_source_train_V1, y_source_train_V1 = train_data_V1\n",
    "x_base_test_V1, y_base_test_V1, x_source_test_V1, y_source_test_V1 = test_data_V1\n",
    "\n",
    "x_base_train_V2, y_base_train_V2, x_source_train_V2, y_source_train_V2 = train_data_V2\n",
    "x_base_test_V2, y_base_test_V2, x_source_test_V2, y_source_test_V2 = test_data_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(x_base_train_V1)\n",
    "interventions_train_V1 = torch.zeros(train_size)\n",
    "interventions_train_V2 = torch.ones(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['three', 'one', 'six'],\n",
       " ['seven', 'zero', 'zero'],\n",
       " ['zero', 'four', 'two'],\n",
       " ['two', 'nine', 'two'],\n",
       " ['six', 'three', 'nine'],\n",
       " ['zero', 'zero', 'five'],\n",
       " ['three', 'two', 'three'],\n",
       " ['nine', 'one', 'eight'],\n",
       " ['four', 'zero', 'four'],\n",
       " ['six', 'two', 'zero'],\n",
       " ['six', 'five', 'three'],\n",
       " ['three', 'one', 'zero'],\n",
       " ['five', 'two', 'four'],\n",
       " ['six', 'one', 'two'],\n",
       " ['six', 'three', 'one'],\n",
       " ['one', 'five', 'nine'],\n",
       " ['one', 'nine', 'two'],\n",
       " ['nine', 'nine', 'five'],\n",
       " ['three', 'five', 'three'],\n",
       " ['eight', 'nine', 'zero'],\n",
       " ['eight', 'one', 'four'],\n",
       " ['zero', 'seven', 'three'],\n",
       " ['zero', 'one', 'three'],\n",
       " ['three', 'five', 'eight'],\n",
       " ['four', 'nine', 'eight'],\n",
       " ['seven', 'nine', 'three'],\n",
       " ['five', 'seven', 'one'],\n",
       " ['six', 'seven', 'six'],\n",
       " ['zero', 'five', 'zero'],\n",
       " ['three', 'eight', 'three'],\n",
       " ['three', 'two', 'five'],\n",
       " ['three', 'zero', 'seven'],\n",
       " ['seven', 'nine', 'seven'],\n",
       " ['five', 'one', 'nine'],\n",
       " ['eight', 'five', 'eight'],\n",
       " ['five', 'six', 'zero'],\n",
       " ['six', 'two', 'two'],\n",
       " ['three', 'two', 'six'],\n",
       " ['three', 'zero', 'four'],\n",
       " ['two', 'eight', 'one'],\n",
       " ['seven', 'six', 'one'],\n",
       " ['four', 'four', 'one'],\n",
       " ['three', 'seven', 'six'],\n",
       " ['zero', 'nine', 'six'],\n",
       " ['seven', 'four', 'one'],\n",
       " ['eight', 'four', 'three'],\n",
       " ['three', 'three', 'four'],\n",
       " ['three', 'five', 'two'],\n",
       " ['four', 'zero', 'two'],\n",
       " ['two', 'seven', 'two'],\n",
       " ['four', 'eight', 'six'],\n",
       " ['three', 'three', 'one'],\n",
       " ['one', 'two', 'six'],\n",
       " ['nine', 'two', 'zero'],\n",
       " ['four', 'nine', 'four'],\n",
       " ['nine', 'one', 'five'],\n",
       " ['six', 'three', 'four'],\n",
       " ['four', 'eight', 'nine'],\n",
       " ['five', 'seven', 'six'],\n",
       " ['nine', 'two', 'two'],\n",
       " ['five', 'nine', 'nine'],\n",
       " ['nine', 'zero', 'seven'],\n",
       " ['zero', 'three', 'zero'],\n",
       " ['zero', 'nine', 'two'],\n",
       " ['five', 'four', 'one'],\n",
       " ['three', 'zero', 'zero'],\n",
       " ['zero', 'six', 'two'],\n",
       " ['three', 'five', 'five'],\n",
       " ['six', 'zero', 'two'],\n",
       " ['zero', 'eight', 'one'],\n",
       " ['five', 'seven', 'seven'],\n",
       " ['seven', 'nine', 'nine'],\n",
       " ['three', 'seven', 'seven'],\n",
       " ['eight', 'six', 'three'],\n",
       " ['four', 'four', 'six'],\n",
       " ['five', 'zero', 'eight'],\n",
       " ['four', 'eight', 'two'],\n",
       " ['two', 'one', 'five'],\n",
       " ['nine', 'three', 'four'],\n",
       " ['one', 'zero', 'one'],\n",
       " ['two', 'six', 'one'],\n",
       " ['five', 'six', 'three'],\n",
       " ['zero', 'two', 'one'],\n",
       " ['three', 'two', 'zero'],\n",
       " ['one', 'six', 'two'],\n",
       " ['seven', 'eight', 'seven'],\n",
       " ['four', 'three', 'zero'],\n",
       " ['three', 'seven', 'nine'],\n",
       " ['one', 'four', 'nine'],\n",
       " ['nine', 'eight', 'zero'],\n",
       " ['two', 'four', 'two'],\n",
       " ['one', 'seven', 'three'],\n",
       " ['one', 'one', 'zero'],\n",
       " ['eight', 'four', 'seven'],\n",
       " ['eight', 'nine', 'one'],\n",
       " ['seven', 'seven', 'two'],\n",
       " ['six', 'seven', 'zero'],\n",
       " ['five', 'seven', 'two'],\n",
       " ['two', 'six', 'four'],\n",
       " ['two', 'eight', 'two'],\n",
       " ['four', 'three', 'seven'],\n",
       " ['two', 'three', 'six'],\n",
       " ['seven', 'one', 'six'],\n",
       " ['seven', 'four', 'five'],\n",
       " ['zero', 'six', 'six'],\n",
       " ['three', 'seven', 'four'],\n",
       " ['seven', 'zero', 'three'],\n",
       " ['seven', 'three', 'three'],\n",
       " ['five', 'six', 'five'],\n",
       " ['six', 'eight', 'three'],\n",
       " ['five', 'seven', 'four'],\n",
       " ['two', 'eight', 'zero'],\n",
       " ['two', 'three', 'two'],\n",
       " ['four', 'three', 'two'],\n",
       " ['eight', 'two', 'seven'],\n",
       " ['seven', 'seven', 'zero'],\n",
       " ['four', 'three', 'five'],\n",
       " ['one', 'five', 'six'],\n",
       " ['five', 'three', 'nine'],\n",
       " ['seven', 'five', 'five'],\n",
       " ['three', 'six', 'eight'],\n",
       " ['one', 'eight', 'four'],\n",
       " ['two', 'five', 'zero'],\n",
       " ['zero', 'five', 'eight'],\n",
       " ['three', 'five', 'zero'],\n",
       " ['four', 'six', 'nine'],\n",
       " ['four', 'three', 'nine'],\n",
       " ['eight', 'five', 'four'],\n",
       " ['five', 'two', 'six'],\n",
       " ['eight', 'four', 'zero'],\n",
       " ['zero', 'one', 'seven'],\n",
       " ['nine', 'eight', 'one'],\n",
       " ['five', 'six', 'one'],\n",
       " ['one', 'eight', 'zero'],\n",
       " ['nine', 'five', 'seven'],\n",
       " ['eight', 'zero', 'five'],\n",
       " ['three', 'four', 'six'],\n",
       " ['three', 'seven', 'three'],\n",
       " ['zero', 'three', 'five'],\n",
       " ['zero', 'zero', 'four'],\n",
       " ['one', 'four', 'seven'],\n",
       " ['seven', 'zero', 'two'],\n",
       " ['eight', 'one', 'nine'],\n",
       " ['three', 'one', 'three'],\n",
       " ['seven', 'nine', 'eight'],\n",
       " ['four', 'two', 'zero'],\n",
       " ['two', 'one', 'one'],\n",
       " ['five', 'one', 'one'],\n",
       " ['four', 'zero', 'three'],\n",
       " ['one', 'five', 'four'],\n",
       " ['eight', 'nine', 'four'],\n",
       " ['one', 'eight', 'eight'],\n",
       " ['five', 'six', 'four'],\n",
       " ['nine', 'eight', 'four'],\n",
       " ['five', 'six', 'eight'],\n",
       " ['six', 'five', 'zero'],\n",
       " ['five', 'three', 'zero'],\n",
       " ['eight', 'zero', 'four'],\n",
       " ['six', 'nine', 'three'],\n",
       " ['three', 'zero', 'five'],\n",
       " ['six', 'five', 'one'],\n",
       " ['seven', 'six', 'nine'],\n",
       " ['six', 'four', 'zero'],\n",
       " ['one', 'nine', 'zero'],\n",
       " ['zero', 'zero', 'two'],\n",
       " ['nine', 'nine', 'seven'],\n",
       " ['four', 'seven', 'zero'],\n",
       " ['nine', 'two', 'four'],\n",
       " ['five', 'three', 'four'],\n",
       " ['one', 'six', 'eight'],\n",
       " ['eight', 'zero', 'zero'],\n",
       " ['eight', 'seven', 'five'],\n",
       " ['one', 'two', 'two'],\n",
       " ['four', 'two', 'nine'],\n",
       " ['seven', 'three', 'four'],\n",
       " ['one', 'two', 'seven'],\n",
       " ['five', 'seven', 'three'],\n",
       " ['eight', 'two', 'six'],\n",
       " ['one', 'six', 'zero'],\n",
       " ['four', 'zero', 'five'],\n",
       " ['two', 'three', 'three'],\n",
       " ['four', 'seven', 'nine'],\n",
       " ['three', 'eight', 'seven'],\n",
       " ['six', 'two', 'nine'],\n",
       " ['one', 'three', 'seven'],\n",
       " ['one', 'five', 'one'],\n",
       " ['six', 'three', 'six'],\n",
       " ['seven', 'zero', 'six'],\n",
       " ['nine', 'five', 'nine'],\n",
       " ['five', 'nine', 'zero'],\n",
       " ['zero', 'zero', 'one'],\n",
       " ['seven', 'seven', 'seven'],\n",
       " ['nine', 'zero', 'one'],\n",
       " ['nine', 'three', 'five'],\n",
       " ['seven', 'six', 'two'],\n",
       " ['six', 'zero', 'five'],\n",
       " ['one', 'three', 'four'],\n",
       " ['three', 'zero', 'three'],\n",
       " ['four', 'one', 'eight'],\n",
       " ['zero', 'eight', 'eight'],\n",
       " ['five', 'two', 'two'],\n",
       " ['three', 'four', 'three'],\n",
       " ['four', 'two', 'six'],\n",
       " ['two', 'four', 'eight'],\n",
       " ['nine', 'zero', 'six'],\n",
       " ['four', 'six', 'one'],\n",
       " ['zero', 'one', 'five'],\n",
       " ['four', 'six', 'two'],\n",
       " ['four', 'one', 'three'],\n",
       " ['two', 'four', 'one'],\n",
       " ['four', 'zero', 'eight'],\n",
       " ['five', 'five', 'seven'],\n",
       " ['six', 'one', 'three'],\n",
       " ['zero', 'six', 'seven'],\n",
       " ['seven', 'one', 'five'],\n",
       " ['four', 'seven', 'four'],\n",
       " ['one', 'one', 'six'],\n",
       " ['four', 'eight', 'one'],\n",
       " ['six', 'four', 'seven'],\n",
       " ['zero', 'nine', 'one'],\n",
       " ['five', 'four', 'five'],\n",
       " ['nine', 'four', 'zero'],\n",
       " ['nine', 'seven', 'six'],\n",
       " ['one', 'seven', 'zero'],\n",
       " ['three', 'six', 'zero'],\n",
       " ['three', 'seven', 'zero'],\n",
       " ['one', 'eight', 'one'],\n",
       " ['four', 'three', 'three'],\n",
       " ['zero', 'three', 'nine'],\n",
       " ['six', 'four', 'one'],\n",
       " ['eight', 'two', 'five'],\n",
       " ['five', 'eight', 'two'],\n",
       " ['nine', 'three', 'nine'],\n",
       " ['six', 'four', 'nine'],\n",
       " ['two', 'eight', 'seven'],\n",
       " ['eight', 'zero', 'six'],\n",
       " ['one', 'six', 'seven'],\n",
       " ['six', 'six', 'nine'],\n",
       " ['one', 'two', 'nine'],\n",
       " ['one', 'nine', 'three'],\n",
       " ['seven', 'five', 'nine'],\n",
       " ['seven', 'four', 'four'],\n",
       " ['zero', 'eight', 'zero'],\n",
       " ['eight', 'eight', 'one'],\n",
       " ['seven', 'five', 'six'],\n",
       " ['eight', 'seven', 'two'],\n",
       " ['two', 'one', 'eight'],\n",
       " ['two', 'six', 'nine'],\n",
       " ['one', 'six', 'five'],\n",
       " ['four', 'two', 'two'],\n",
       " ['eight', 'one', 'five'],\n",
       " ['six', 'five', 'eight'],\n",
       " ['nine', 'seven', 'eight'],\n",
       " ['five', 'one', 'zero'],\n",
       " ['three', 'nine', 'five'],\n",
       " ['two', 'zero', 'zero'],\n",
       " ['four', 'five', 'four'],\n",
       " ['six', 'five', 'five'],\n",
       " ['one', 'zero', 'zero'],\n",
       " ['five', 'zero', 'nine'],\n",
       " ['five', 'eight', 'four'],\n",
       " ['six', 'eight', 'six'],\n",
       " ['six', 'eight', 'five'],\n",
       " ['five', 'two', 'three'],\n",
       " ['three', 'four', 'zero'],\n",
       " ['seven', 'five', 'three'],\n",
       " ['eight', 'eight', 'zero'],\n",
       " ['four', 'four', 'four'],\n",
       " ['eight', 'five', 'one'],\n",
       " ['one', 'four', 'zero'],\n",
       " ['six', 'nine', 'six'],\n",
       " ['five', 'one', 'eight'],\n",
       " ['zero', 'six', 'four'],\n",
       " ['nine', 'three', 'two'],\n",
       " ['three', 'zero', 'one'],\n",
       " ['seven', 'one', 'seven'],\n",
       " ['four', 'five', 'one'],\n",
       " ['four', 'zero', 'nine'],\n",
       " ['three', 'eight', 'five'],\n",
       " ['two', 'two', 'six'],\n",
       " ['two', 'eight', 'five'],\n",
       " ['zero', 'three', 'six'],\n",
       " ['eight', 'seven', 'seven'],\n",
       " ['seven', 'three', 'five'],\n",
       " ['five', 'four', 'nine'],\n",
       " ['two', 'four', 'three'],\n",
       " ['four', 'nine', 'one'],\n",
       " ['zero', 'three', 'two'],\n",
       " ['one', 'zero', 'four'],\n",
       " ['four', 'one', 'two'],\n",
       " ['two', 'zero', 'nine'],\n",
       " ['zero', 'seven', 'nine'],\n",
       " ['two', 'four', 'zero'],\n",
       " ['five', 'three', 'five'],\n",
       " ['three', 'nine', 'four'],\n",
       " ['two', 'two', 'zero'],\n",
       " ['seven', 'three', 'one'],\n",
       " ['two', 'nine', 'seven'],\n",
       " ['five', 'four', 'two'],\n",
       " ['six', 'zero', 'zero'],\n",
       " ['one', 'two', 'eight'],\n",
       " ['six', 'three', 'five'],\n",
       " ['six', 'four', 'four'],\n",
       " ['zero', 'nine', 'three'],\n",
       " ['one', 'three', 'nine'],\n",
       " ['two', 'six', 'seven'],\n",
       " ['six', 'five', 'six'],\n",
       " ['nine', 'zero', 'two'],\n",
       " ['eight', 'six', 'four'],\n",
       " ['four', 'five', 'seven'],\n",
       " ['one', 'four', 'three'],\n",
       " ['two', 'five', 'nine'],\n",
       " ['eight', 'seven', 'six'],\n",
       " ['six', 'six', 'three'],\n",
       " ['nine', 'six', 'four'],\n",
       " ['zero', 'two', 'eight'],\n",
       " ['one', 'seven', 'two'],\n",
       " ['two', 'one', 'nine'],\n",
       " ['four', 'five', 'five'],\n",
       " ['seven', 'one', 'four'],\n",
       " ['eight', 'zero', 'three'],\n",
       " ['one', 'seven', 'eight'],\n",
       " ['five', 'nine', 'eight'],\n",
       " ['seven', 'two', 'eight'],\n",
       " ['six', 'one', 'nine'],\n",
       " ['five', 'eight', 'five'],\n",
       " ['eight', 'eight', 'six'],\n",
       " ['one', 'four', 'two'],\n",
       " ['three', 'nine', 'two'],\n",
       " ['seven', 'three', 'nine'],\n",
       " ['five', 'one', 'five'],\n",
       " ['four', 'two', 'five'],\n",
       " ['seven', 'two', 'seven'],\n",
       " ['three', 'eight', 'six'],\n",
       " ['five', 'seven', 'five'],\n",
       " ['nine', 'five', 'two'],\n",
       " ['eight', 'eight', 'two'],\n",
       " ['two', 'seven', 'one'],\n",
       " ['zero', 'zero', 'zero'],\n",
       " ['seven', 'nine', 'one'],\n",
       " ['eight', 'six', 'six'],\n",
       " ['one', 'eight', 'three'],\n",
       " ['eight', 'five', 'zero'],\n",
       " ['eight', 'seven', 'three'],\n",
       " ['two', 'three', 'seven'],\n",
       " ['zero', 'five', 'one'],\n",
       " ['zero', 'one', 'eight'],\n",
       " ['six', 'zero', 'eight'],\n",
       " ['eight', 'one', 'six'],\n",
       " ['seven', 'five', 'zero'],\n",
       " ['four', 'three', 'one'],\n",
       " ['two', 'five', 'eight'],\n",
       " ['one', 'one', 'two'],\n",
       " ['one', 'zero', 'three'],\n",
       " ['eight', 'three', 'zero'],\n",
       " ['five', 'nine', 'four'],\n",
       " ['eight', 'three', 'six'],\n",
       " ['six', 'eight', 'zero'],\n",
       " ['two', 'seven', 'six'],\n",
       " ['zero', 'zero', 'three'],\n",
       " ['eight', 'zero', 'eight'],\n",
       " ['three', 'two', 'two'],\n",
       " ['nine', 'eight', 'seven'],\n",
       " ['nine', 'nine', 'three'],\n",
       " ['nine', 'zero', 'three'],\n",
       " ['four', 'five', 'three'],\n",
       " ['five', 'three', 'eight'],\n",
       " ['one', 'eight', 'seven'],\n",
       " ['six', 'one', 'four'],\n",
       " ['zero', 'four', 'seven'],\n",
       " ['three', 'one', 'five'],\n",
       " ['zero', 'two', 'three'],\n",
       " ['one', 'one', 'four'],\n",
       " ['nine', 'zero', 'eight'],\n",
       " ['zero', 'seven', 'two'],\n",
       " ['six', 'one', 'seven'],\n",
       " ['seven', 'eight', 'two'],\n",
       " ['six', 'seven', 'four'],\n",
       " ['six', 'eight', 'seven'],\n",
       " ['six', 'seven', 'seven'],\n",
       " ['nine', 'seven', 'five'],\n",
       " ['five', 'five', 'four'],\n",
       " ['eight', 'seven', 'nine'],\n",
       " ['six', 'six', 'zero'],\n",
       " ['zero', 'seven', 'seven'],\n",
       " ['eight', 'six', 'zero'],\n",
       " ['four', 'nine', 'six'],\n",
       " ['three', 'nine', 'zero'],\n",
       " ['two', 'two', 'one'],\n",
       " ['zero', 'six', 'eight'],\n",
       " ['one', 'three', 'two'],\n",
       " ['three', 'nine', 'eight'],\n",
       " ['four', 'nine', 'three'],\n",
       " ['seven', 'six', 'three'],\n",
       " ['three', 'eight', 'one'],\n",
       " ['five', 'zero', 'three'],\n",
       " ['five', 'eight', 'six'],\n",
       " ['three', 'eight', 'zero'],\n",
       " ['zero', 'four', 'three'],\n",
       " ['nine', 'nine', 'eight'],\n",
       " ['four', 'five', 'eight'],\n",
       " ['five', 'five', 'zero'],\n",
       " ['one', 'zero', 'nine'],\n",
       " ['eight', 'eight', 'four'],\n",
       " ['seven', 'seven', 'one'],\n",
       " ['two', 'seven', 'zero'],\n",
       " ['one', 'five', 'zero'],\n",
       " ['three', 'zero', 'eight'],\n",
       " ['four', 'four', 'eight'],\n",
       " ['two', 'five', 'seven'],\n",
       " ['one', 'four', 'one'],\n",
       " ['two', 'nine', 'eight'],\n",
       " ['eight', 'seven', 'eight'],\n",
       " ['four', 'zero', 'zero'],\n",
       " ['five', 'nine', 'three'],\n",
       " ['two', 'two', 'three'],\n",
       " ['nine', 'eight', 'five'],\n",
       " ['seven', 'six', 'six'],\n",
       " ['three', 'one', 'two'],\n",
       " ['four', 'six', 'seven'],\n",
       " ['zero', 'three', 'four'],\n",
       " ['one', 'eight', 'five'],\n",
       " ['five', 'nine', 'six'],\n",
       " ['three', 'three', 'six'],\n",
       " ['two', 'two', 'eight'],\n",
       " ['eight', 'nine', 'three'],\n",
       " ['nine', 'five', 'three'],\n",
       " ['three', 'three', 'seven'],\n",
       " ['six', 'five', 'nine'],\n",
       " ['eight', 'six', 'nine'],\n",
       " ['seven', 'three', 'eight'],\n",
       " ['three', 'five', 'six'],\n",
       " ['four', 'six', 'four'],\n",
       " ['six', 'five', 'four'],\n",
       " ['five', 'eight', 'one'],\n",
       " ['two', 'zero', 'eight'],\n",
       " ['one', 'seven', 'six'],\n",
       " ['eight', 'four', 'six'],\n",
       " ['zero', 'zero', 'six'],\n",
       " ['nine', 'seven', 'three'],\n",
       " ['three', 'three', 'eight'],\n",
       " ['one', 'six', 'six'],\n",
       " ['zero', 'eight', 'five'],\n",
       " ['six', 'two', 'seven'],\n",
       " ['one', 'nine', 'nine'],\n",
       " ['four', 'nine', 'seven'],\n",
       " ['three', 'six', 'seven'],\n",
       " ['eight', 'nine', 'six'],\n",
       " ['seven', 'eight', 'zero'],\n",
       " ['eight', 'nine', 'seven'],\n",
       " ['five', 'three', 'six'],\n",
       " ['seven', 'three', 'six'],\n",
       " ['eight', 'eight', 'nine'],\n",
       " ['one', 'three', 'eight'],\n",
       " ['five', 'eight', 'eight'],\n",
       " ['zero', 'two', 'zero'],\n",
       " ['one', 'zero', 'six'],\n",
       " ['seven', 'five', 'two'],\n",
       " ['four', 'eight', 'five'],\n",
       " ['four', 'eight', 'eight'],\n",
       " ['one', 'three', 'six'],\n",
       " ['zero', 'six', 'one'],\n",
       " ['zero', 'two', 'two'],\n",
       " ['one', 'zero', 'seven'],\n",
       " ['seven', 'three', 'two'],\n",
       " ['three', 'four', 'four'],\n",
       " ['nine', 'eight', 'three'],\n",
       " ['four', 'seven', 'seven'],\n",
       " ['nine', 'zero', 'nine'],\n",
       " ['nine', 'one', 'six'],\n",
       " ['one', 'two', 'one'],\n",
       " ['zero', 'one', 'zero'],\n",
       " ['one', 'two', 'three'],\n",
       " ['zero', 'three', 'three'],\n",
       " ['one', 'two', 'five'],\n",
       " ['nine', 'eight', 'eight'],\n",
       " ['three', 'three', 'three'],\n",
       " ['four', 'one', 'seven'],\n",
       " ['one', 'one', 'nine'],\n",
       " ['seven', 'eight', 'eight'],\n",
       " ['two', 'four', 'nine'],\n",
       " ['eight', 'nine', 'five'],\n",
       " ['nine', 'nine', 'zero'],\n",
       " ['three', 'eight', 'nine'],\n",
       " ['eight', 'three', 'nine'],\n",
       " ['eight', 'eight', 'three'],\n",
       " ['two', 'nine', 'zero'],\n",
       " ['seven', 'eight', 'three'],\n",
       " ['six', 'zero', 'three'],\n",
       " ['zero', 'four', 'eight'],\n",
       " ['eight', 'eight', 'seven'],\n",
       " ['nine', 'four', 'three'],\n",
       " ['three', 'seven', 'eight'],\n",
       " ['three', 'six', 'five'],\n",
       " ['zero', 'six', 'zero'],\n",
       " ['nine', 'seven', 'zero'],\n",
       " ['seven', 'seven', 'five'],\n",
       " ['seven', 'eight', 'one'],\n",
       " ['seven', 'four', 'six'],\n",
       " ['seven', 'four', 'two'],\n",
       " ['one', 'five', 'two'],\n",
       " ['nine', 'eight', 'nine'],\n",
       " ['seven', 'two', 'nine'],\n",
       " ['five', 'two', 'eight'],\n",
       " ['two', 'seven', 'five'],\n",
       " ['zero', 'five', 'six'],\n",
       " ['seven', 'nine', 'two'],\n",
       " ['zero', 'one', 'one'],\n",
       " ['four', 'seven', 'five'],\n",
       " ['nine', 'eight', 'six'],\n",
       " ['one', 'nine', 'four'],\n",
       " ['nine', 'four', 'one'],\n",
       " ['four', 'one', 'four'],\n",
       " ['three', 'nine', 'three'],\n",
       " ['four', 'four', 'nine'],\n",
       " ['three', 'five', 'nine'],\n",
       " ['four', 'seven', 'six'],\n",
       " ['three', 'one', 'seven'],\n",
       " ['one', 'six', 'four'],\n",
       " ['nine', 'seven', 'nine'],\n",
       " ['seven', 'zero', 'nine'],\n",
       " ['five', 'five', 'nine'],\n",
       " ['five', 'one', 'four'],\n",
       " ['eight', 'three', 'eight'],\n",
       " ['zero', 'four', 'four'],\n",
       " ['seven', 'eight', 'five'],\n",
       " ['two', 'five', 'six'],\n",
       " ['four', 'zero', 'seven'],\n",
       " ['three', 'four', 'one'],\n",
       " ['zero', 'five', 'four'],\n",
       " ['eight', 'six', 'two'],\n",
       " ['one', 'six', 'one'],\n",
       " ['five', 'two', 'zero'],\n",
       " ['nine', 'three', 'six'],\n",
       " ['four', 'nine', 'zero'],\n",
       " ['five', 'seven', 'eight'],\n",
       " ['nine', 'one', 'one'],\n",
       " ['six', 'zero', 'four'],\n",
       " ['nine', 'six', 'eight'],\n",
       " ['seven', 'two', 'six'],\n",
       " ['two', 'seven', 'nine'],\n",
       " ['two', 'two', 'two'],\n",
       " ['five', 'seven', 'zero'],\n",
       " ['nine', 'six', 'nine'],\n",
       " ['five', 'six', 'nine'],\n",
       " ['nine', 'two', 'six'],\n",
       " ['eight', 'three', 'five'],\n",
       " ['seven', 'zero', 'eight'],\n",
       " ['seven', 'seven', 'nine'],\n",
       " ['seven', 'eight', 'six'],\n",
       " ['two', 'five', 'three'],\n",
       " ['six', 'two', 'four'],\n",
       " ['two', 'eight', 'four'],\n",
       " ['eight', 'six', 'seven'],\n",
       " ['six', 'seven', 'one'],\n",
       " ['eight', 'four', 'eight'],\n",
       " ['six', 'two', 'eight'],\n",
       " ['zero', 'eight', 'three'],\n",
       " ['zero', 'one', 'four'],\n",
       " ['two', 'eight', 'eight'],\n",
       " ['four', 'zero', 'one'],\n",
       " ['three', 'two', 'four'],\n",
       " ['six', 'four', 'five'],\n",
       " ['five', 'four', 'four'],\n",
       " ['nine', 'four', 'two'],\n",
       " ['eight', 'eight', 'eight'],\n",
       " ['three', 'five', 'one'],\n",
       " ['one', 'one', 'one'],\n",
       " ['five', 'zero', 'five'],\n",
       " ['five', 'two', 'five'],\n",
       " ['five', 'five', 'two'],\n",
       " ['two', 'seven', 'seven'],\n",
       " ['two', 'four', 'six'],\n",
       " ['six', 'eight', 'four'],\n",
       " ['seven', 'zero', 'five'],\n",
       " ['eight', 'three', 'four'],\n",
       " ['nine', 'five', 'one'],\n",
       " ['eight', 'six', 'five'],\n",
       " ['zero', 'two', 'six'],\n",
       " ['one', 'six', 'three'],\n",
       " ['four', 'two', 'seven'],\n",
       " ['eight', 'six', 'eight'],\n",
       " ['five', 'two', 'one'],\n",
       " ['three', 'four', 'eight'],\n",
       " ['nine', 'six', 'two'],\n",
       " ['six', 'six', 'eight'],\n",
       " ['three', 'five', 'four'],\n",
       " ['zero', 'eight', 'four'],\n",
       " ['two', 'seven', 'three'],\n",
       " ['three', 'zero', 'six'],\n",
       " ['five', 'seven', 'nine'],\n",
       " ['zero', 'one', 'two'],\n",
       " ['zero', 'zero', 'eight'],\n",
       " ['five', 'three', 'two'],\n",
       " ['six', 'eight', 'two'],\n",
       " ['eight', 'five', 'three'],\n",
       " ['eight', 'two', 'eight'],\n",
       " ['five', 'nine', 'seven'],\n",
       " ['zero', 'eight', 'nine'],\n",
       " ['two', 'one', 'two'],\n",
       " ['zero', 'eight', 'seven'],\n",
       " ['four', 'eight', 'four'],\n",
       " ['six', 'nine', 'one'],\n",
       " ['two', 'one', 'three'],\n",
       " ['seven', 'eight', 'nine'],\n",
       " ['five', 'eight', 'nine'],\n",
       " ['eight', 'one', 'eight'],\n",
       " ['eight', 'one', 'seven'],\n",
       " ['six', 'zero', 'one'],\n",
       " ['seven', 'zero', 'four'],\n",
       " ['two', 'seven', 'eight'],\n",
       " ['four', 'one', 'six'],\n",
       " ['eight', 'zero', 'two'],\n",
       " ['zero', 'three', 'one'],\n",
       " ['eight', 'two', 'nine'],\n",
       " ['zero', 'four', 'five'],\n",
       " ['three', 'zero', 'two'],\n",
       " ['five', 'nine', 'two'],\n",
       " ['four', 'four', 'five'],\n",
       " ['six', 'three', 'eight'],\n",
       " ['zero', 'seven', 'four'],\n",
       " ['five', 'zero', 'two'],\n",
       " ['two', 'nine', 'nine'],\n",
       " ['one', 'eight', 'six'],\n",
       " ['three', 'two', 'seven'],\n",
       " ['seven', 'zero', 'seven'],\n",
       " ['eight', 'seven', 'one'],\n",
       " ['four', 'two', 'three'],\n",
       " ['nine', 'one', 'zero'],\n",
       " ['nine', 'nine', 'six'],\n",
       " ['four', 'eight', 'three'],\n",
       " ['eight', 'three', 'seven'],\n",
       " ['four', 'four', 'three'],\n",
       " ['nine', 'two', 'eight'],\n",
       " ['five', 'four', 'six'],\n",
       " ['six', 'six', 'seven'],\n",
       " ['seven', 'six', 'eight'],\n",
       " ['six', 'six', 'two'],\n",
       " ['six', 'four', 'eight'],\n",
       " ['one', 'four', 'four'],\n",
       " ['four', 'four', 'seven'],\n",
       " ['nine', 'zero', 'five'],\n",
       " ['nine', 'six', 'zero'],\n",
       " ['four', 'five', 'zero'],\n",
       " ['five', 'two', 'seven'],\n",
       " ['three', 'six', 'three'],\n",
       " ['two', 'zero', 'one'],\n",
       " ['seven', 'one', 'eight'],\n",
       " ['six', 'nine', 'five'],\n",
       " ['zero', 'four', 'zero'],\n",
       " ['one', 'four', 'five'],\n",
       " ['six', 'three', 'three'],\n",
       " ['zero', 'four', 'nine'],\n",
       " ['three', 'nine', 'nine'],\n",
       " ['two', 'eight', 'three'],\n",
       " ['nine', 'five', 'eight'],\n",
       " ['nine', 'five', 'six'],\n",
       " ['five', 'five', 'six'],\n",
       " ['one', 'eight', 'nine'],\n",
       " ['one', 'seven', 'five'],\n",
       " ['zero', 'seven', 'five'],\n",
       " ['six', 'three', 'zero'],\n",
       " ['zero', 'two', 'seven'],\n",
       " ['four', 'seven', 'one'],\n",
       " ['two', 'three', 'one'],\n",
       " ['eight', 'five', 'two'],\n",
       " ['two', 'five', 'one'],\n",
       " ['three', 'one', 'nine'],\n",
       " ['nine', 'one', 'nine'],\n",
       " ['six', 'nine', 'two'],\n",
       " ['eight', 'four', 'two'],\n",
       " ['one', 'two', 'zero'],\n",
       " ['two', 'six', 'five'],\n",
       " ['six', 'three', 'seven'],\n",
       " ['nine', 'six', 'six'],\n",
       " ['four', 'one', 'zero'],\n",
       " ['three', 'three', 'nine'],\n",
       " ['six', 'seven', 'two'],\n",
       " ['two', 'six', 'eight'],\n",
       " ['two', 'nine', 'six'],\n",
       " ['four', 'three', 'six'],\n",
       " ['nine', 'nine', 'four'],\n",
       " ['two', 'three', 'eight'],\n",
       " ['four', 'four', 'zero'],\n",
       " ['one', 'four', 'six'],\n",
       " ['seven', 'five', 'eight'],\n",
       " ['three', 'one', 'four'],\n",
       " ['zero', 'nine', 'eight'],\n",
       " ['five', 'five', 'five'],\n",
       " ['four', 'six', 'eight'],\n",
       " ['four', 'four', 'two'],\n",
       " ['three', 'three', 'five'],\n",
       " ['zero', 'three', 'seven'],\n",
       " ['one', 'five', 'seven'],\n",
       " ['zero', 'nine', 'five'],\n",
       " ['eight', 'two', 'zero'],\n",
       " ['eight', 'one', 'two'],\n",
       " ['seven', 'one', 'one'],\n",
       " ['three', 'four', 'nine'],\n",
       " ['six', 'four', 'two'],\n",
       " ['one', 'one', 'eight'],\n",
       " ['six', 'eight', 'one'],\n",
       " ['nine', 'four', 'nine'],\n",
       " ['eight', 'nine', 'two'],\n",
       " ['two', 'four', 'four'],\n",
       " ['five', 'four', 'eight'],\n",
       " ['two', 'five', 'four'],\n",
       " ['zero', 'seven', 'zero'],\n",
       " ['seven', 'one', 'nine'],\n",
       " ['seven', 'nine', 'five'],\n",
       " ['three', 'six', 'nine'],\n",
       " ['four', 'one', 'five'],\n",
       " ['four', 'eight', 'seven'],\n",
       " ['nine', 'three', 'eight'],\n",
       " ['seven', 'one', 'two'],\n",
       " ['three', 'six', 'two'],\n",
       " ['six', 'six', 'five'],\n",
       " ['four', 'eight', 'zero'],\n",
       " ['two', 'zero', 'six'],\n",
       " ['two', 'one', 'six'],\n",
       " ['five', 'eight', 'three'],\n",
       " ['three', 'four', 'two'],\n",
       " ['nine', 'five', 'four'],\n",
       " ['one', 'two', 'four'],\n",
       " ['four', 'five', 'nine'],\n",
       " ['eight', 'one', 'zero'],\n",
       " ['six', 'one', 'six'],\n",
       " ['four', 'one', 'nine'],\n",
       " ['six', 'two', 'five'],\n",
       " ['seven', 'four', 'eight'],\n",
       " ['nine', 'seven', 'two'],\n",
       " ['two', 'nine', 'three'],\n",
       " ['six', 'nine', 'four'],\n",
       " ['seven', 'six', 'seven'],\n",
       " ['four', 'two', 'four'],\n",
       " ['six', 'six', 'one'],\n",
       " ['four', 'zero', 'six'],\n",
       " ['two', 'six', 'two'],\n",
       " ['seven', 'five', 'four'],\n",
       " ['zero', 'eight', 'six'],\n",
       " ['nine', 'eight', 'two'],\n",
       " ['two', 'two', 'four'],\n",
       " ['five', 'four', 'three'],\n",
       " ['nine', 'two', 'seven'],\n",
       " ['one', 'seven', 'four'],\n",
       " ['seven', 'six', 'zero'],\n",
       " ['four', 'seven', 'eight'],\n",
       " ['five', 'zero', 'one'],\n",
       " ['eight', 'seven', 'four'],\n",
       " ['six', 'one', 'eight'],\n",
       " ['six', 'six', 'six'],\n",
       " ['zero', 'seven', 'eight'],\n",
       " ['two', 'eight', 'nine'],\n",
       " ['zero', 'six', 'nine'],\n",
       " ['nine', 'one', 'three'],\n",
       " ['eight', 'three', 'three'],\n",
       " ['seven', 'nine', 'zero'],\n",
       " ['seven', 'two', 'zero'],\n",
       " ['six', 'seven', 'eight'],\n",
       " ['seven', 'six', 'five'],\n",
       " ['five', 'four', 'zero'],\n",
       " ['three', 'eight', 'four'],\n",
       " ['five', 'five', 'eight'],\n",
       " ['five', 'zero', 'seven'],\n",
       " ['two', 'zero', 'four'],\n",
       " ['eight', 'two', 'four'],\n",
       " ['one', 'three', 'one'],\n",
       " ['nine', 'five', 'five'],\n",
       " ['five', 'two', 'nine'],\n",
       " ['four', 'nine', 'two'],\n",
       " ['one', 'five', 'eight'],\n",
       " ['one', 'eight', 'two'],\n",
       " ['one', 'seven', 'seven'],\n",
       " ['eight', 'nine', 'eight'],\n",
       " ['one', 'zero', 'eight'],\n",
       " ['one', 'zero', 'two'],\n",
       " ['two', 'three', 'five'],\n",
       " ['five', 'six', 'two'],\n",
       " ['two', 'zero', 'seven'],\n",
       " ['eight', 'eight', 'five'],\n",
       " ['zero', 'five', 'five'],\n",
       " ['zero', 'four', 'one'],\n",
       " ['one', 'nine', 'eight'],\n",
       " ['eight', 'five', 'seven'],\n",
       " ['six', 'four', 'three'],\n",
       " ['three', 'seven', 'one'],\n",
       " ['eight', 'three', 'two'],\n",
       " ['seven', 'two', 'four'],\n",
       " ['four', 'two', 'one'],\n",
       " ['eight', 'zero', 'seven'],\n",
       " ['zero', 'two', 'four'],\n",
       " ['six', 'two', 'six'],\n",
       " ['nine', 'two', 'five'],\n",
       " ['one', 'zero', 'five'],\n",
       " ['zero', 'zero', 'seven'],\n",
       " ['nine', 'nine', 'two'],\n",
       " ['one', 'nine', 'one'],\n",
       " ['seven', 'eight', 'four'],\n",
       " ['nine', 'four', 'eight'],\n",
       " ['nine', 'one', 'four'],\n",
       " ['zero', 'three', 'eight'],\n",
       " ['six', 'zero', 'nine'],\n",
       " ['one', 'six', 'nine'],\n",
       " ['seven', 'four', 'nine'],\n",
       " ['two', 'one', 'zero'],\n",
       " ['nine', 'three', 'seven'],\n",
       " ['nine', 'six', 'seven'],\n",
       " ['three', 'four', 'five'],\n",
       " ['three', 'nine', 'seven'],\n",
       " ['six', 'two', 'one'],\n",
       " ['zero', 'one', 'nine'],\n",
       " ['eight', 'two', 'two'],\n",
       " ['eight', 'four', 'one'],\n",
       " ['five', 'one', 'seven'],\n",
       " ['two', 'three', 'four'],\n",
       " ['eight', 'zero', 'nine'],\n",
       " ['six', 'one', 'zero'],\n",
       " ['six', 'seven', 'nine'],\n",
       " ['four', 'six', 'five'],\n",
       " ['zero', 'seven', 'one'],\n",
       " ['three', 'three', 'two'],\n",
       " ['five', 'eight', 'zero'],\n",
       " ['zero', 'nine', 'four'],\n",
       " ['five', 'one', 'six'],\n",
       " ['seven', 'four', 'zero'],\n",
       " ['two', 'zero', 'five'],\n",
       " ['seven', 'seven', 'four'],\n",
       " ['zero', 'five', 'seven'],\n",
       " ['seven', 'four', 'three'],\n",
       " ['two', 'six', 'zero'],\n",
       " ['six', 'three', 'two'],\n",
       " ['five', 'zero', 'six'],\n",
       " ['six', 'six', 'four'],\n",
       " ['six', 'nine', 'seven'],\n",
       " ['zero', 'four', 'six'],\n",
       " ['three', 'three', 'zero'],\n",
       " ['zero', 'five', 'nine'],\n",
       " ['eight', 'one', 'three'],\n",
       " ['zero', 'nine', 'seven'],\n",
       " ['two', 'seven', 'four'],\n",
       " ['five', 'five', 'three'],\n",
       " ['eight', 'two', 'three'],\n",
       " ['nine', 'six', 'one'],\n",
       " ['six', 'two', 'three'],\n",
       " ['six', 'seven', 'five'],\n",
       " ['two', 'two', 'seven'],\n",
       " ['eight', 'five', 'six'],\n",
       " ['one', 'nine', 'five'],\n",
       " ['seven', 'three', 'zero'],\n",
       " ['nine', 'nine', 'one'],\n",
       " ['eight', 'two', 'one'],\n",
       " ['one', 'five', 'five'],\n",
       " ['seven', 'seven', 'eight'],\n",
       " ['seven', 'six', 'four'],\n",
       " ['zero', 'six', 'three'],\n",
       " ['four', 'nine', 'nine'],\n",
       " ['three', 'nine', 'six'],\n",
       " ['nine', 'two', 'one'],\n",
       " ['two', 'two', 'five'],\n",
       " ['two', 'nine', 'one'],\n",
       " ['two', 'six', 'six'],\n",
       " ['three', 'six', 'one'],\n",
       " ['two', 'four', 'seven'],\n",
       " ['five', 'three', 'three'],\n",
       " ['four', 'seven', 'two'],\n",
       " ['zero', 'five', 'three'],\n",
       " ['nine', 'five', 'zero'],\n",
       " ['two', 'three', 'nine'],\n",
       " ['one', 'three', 'five'],\n",
       " ['zero', 'one', 'six'],\n",
       " ['six', 'five', 'two'],\n",
       " ['three', 'nine', 'one'],\n",
       " ['five', 'nine', 'one'],\n",
       " ['five', 'one', 'three'],\n",
       " ['nine', 'six', 'five'],\n",
       " ['two', 'zero', 'two'],\n",
       " ['six', 'one', 'five'],\n",
       " ['seven', 'seven', 'six'],\n",
       " ['two', 'five', 'five'],\n",
       " ['three', 'six', 'six'],\n",
       " ['seven', 'two', 'two'],\n",
       " ['nine', 'four', 'six'],\n",
       " ['four', 'three', 'eight'],\n",
       " ['two', 'one', 'seven'],\n",
       " ['one', 'four', 'eight'],\n",
       " ['six', 'nine', 'nine'],\n",
       " ['seven', 'two', 'five'],\n",
       " ['five', 'six', 'seven'],\n",
       " ['eight', 'zero', 'one'],\n",
       " ['six', 'seven', 'three'],\n",
       " ['eight', 'five', 'five'],\n",
       " ['zero', 'two', 'five'],\n",
       " ['two', 'four', 'five'],\n",
       " ['four', 'seven', 'three'],\n",
       " ['seven', 'two', 'one'],\n",
       " ['one', 'one', 'seven'],\n",
       " ['nine', 'zero', 'four'],\n",
       " ['two', 'one', 'four'],\n",
       " ['four', 'three', 'four'],\n",
       " ['six', 'five', 'seven']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_base_train_V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base_train_both = np.concatenate([np.array(x_base_train_V1),\n",
    "                                np.array(x_base_train_V2)], axis=0)\n",
    "\n",
    "X_sources_train_both = [np.concatenate([np.array(x_source_train_V1),\n",
    "                            np.array(x_source_train_V2)], axis=0)]\n",
    "\n",
    "y_base_train_both = np.concatenate([np.array(y_base_train_V1),\n",
    "                            np.array(y_base_train_V2)], axis=0)\n",
    "\n",
    "y_source_train_both = np.concatenate([np.array(y_source_train_V1),\n",
    "                            np.array(y_source_train_V2)])\n",
    "\n",
    "interventions_train_both = np.concatenate([np.array(interventions_train_V1),\n",
    "                            np.array(interventions_train_V2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = 0\n",
    "V2 = 1\n",
    "embedding_dim = 5\n",
    "\n",
    "id_to_coords = {\n",
    "    V1: [{\"layer\": 1, \"start\": 0, \"end\": embedding_dim}], \n",
    "    V2: [{\"layer\": 1, \"start\": embedding_dim, \"end\": embedding_dim*2}]    \n",
    "}\n",
    "\n",
    "both_model = TorchDeepNeuralClassifierIIT(\n",
    "    hidden_dim=embedding_dim*4, \n",
    "    hidden_activation=torch.nn.ReLU(), \n",
    "    num_layers=2, \n",
    "    id_to_coords=id_to_coords, \n",
    "    vocab=vals,\n",
    "    output_size=output_size,\n",
    "    num_inputs=num_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/example_test.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/example_test.ipynb#ch0000035?line=0'>1</a>\u001b[0m _ \u001b[39m=\u001b[39m both_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/example_test.ipynb#ch0000035?line=1'>2</a>\u001b[0m     X_base_train_both, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/example_test.ipynb#ch0000035?line=2'>3</a>\u001b[0m     X_sources_train_both, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/example_test.ipynb#ch0000035?line=3'>4</a>\u001b[0m     y_base_train_both, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/example_test.ipynb#ch0000035?line=4'>5</a>\u001b[0m     y_source_train_both, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/example_test.ipynb#ch0000035?line=5'>6</a>\u001b[0m     interventions_train_both)\n",
      "File \u001b[0;32m~/Documents/work/CSLI/iit-distribution-rule/./cs224u/torch_model_base.py:356\u001b[0m, in \u001b[0;36mTorchModelBase.fit\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/./cs224u/torch_model_base.py?line=352'>353</a>\u001b[0m X_batch \u001b[39m=\u001b[39m batch[: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/./cs224u/torch_model_base.py?line=353'>354</a>\u001b[0m y_batch \u001b[39m=\u001b[39m batch[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/./cs224u/torch_model_base.py?line=355'>356</a>\u001b[0m batch_preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49mX_batch)\n\u001b[1;32m    <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/./cs224u/torch_model_base.py?line=357'>358</a>\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(batch_preds, y_batch)\n\u001b[1;32m    <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/./cs224u/torch_model_base.py?line=359'>360</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/./cs224u/torch_model_base.py?line=360'>361</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mreduction \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/work/CSLI/iit-distribution-rule/model_iit.py:43\u001b[0m, in \u001b[0;36mIITModel.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/model_iit.py?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, get \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(gets[layer]):\n\u001b[1;32m     <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/model_iit.py?line=41'>42</a>\u001b[0m     handlers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gets_sets(gets \u001b[39m=\u001b[39m{layer: [get]},sets \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m---> <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/model_iit.py?line=42'>43</a>\u001b[0m     source_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mno_IIT_forward(sources[i])\n\u001b[1;32m     <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/model_iit.py?line=43'>44</a>\u001b[0m     \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m     <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/model_iit.py?line=44'>45</a>\u001b[0m         handler\u001b[39m.\u001b[39mremove()\n",
      "File \u001b[0;32m~/Documents/work/CSLI/iit-distribution-rule/model_iit.py:28\u001b[0m, in \u001b[0;36mIITModel.no_IIT_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/model_iit.py?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mno_IIT_forward\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m---> <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/model_iit.py?line=27'>28</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/work/CSLI/iit-distribution-rule/model.py:72\u001b[0m, in \u001b[0;36mTorchDeepNeuralModel.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/model.py?line=70'>71</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m---> <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/model.py?line=71'>72</a>\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(X)\n\u001b[1;32m     <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/model.py?line=72'>73</a>\u001b[0m     new_x \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='file:///home/jamesflemings/Documents/work/CSLI/iit-distribution-rule/model.py?line=73'>74</a>\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=156'>157</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=157'>158</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=158'>159</a>\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/modules/sparse.py?line=159'>160</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/functional.py:2044\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/functional.py?line=2037'>2038</a>\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/functional.py?line=2038'>2039</a>\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/functional.py?line=2039'>2040</a>\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/functional.py?line=2040'>2041</a>\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/functional.py?line=2041'>2042</a>\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/functional.py?line=2042'>2043</a>\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> <a href='file:///home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/torch/nn/functional.py?line=2043'>2044</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "_ = both_model.fit(\n",
    "    X_base_train_both, \n",
    "    X_sources_train_both, \n",
    "    y_base_train_both, \n",
    "    y_source_train_both, \n",
    "    interventions_train_both)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23b0dade77de706664b673ec0b749cccbdb5cac5bb41527ce15ce25b9a737383"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
