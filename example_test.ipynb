{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from iit import generate_data, get_iit_distribution_dataset_both\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from model_classifier import TorchDeepNeuralEmbeddingClassifier\n",
    "from model_regression import TorchLinearEmbeddingRegression\n",
    "import model_classifier_iit\n",
    "from model_classifier_iit import TorchDeepNeuralClassifierIIT\n",
    "from model_regression_iit import TorchLinearEmbeddingRegressionIIT\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import vsm\n",
    "from sklearn.metrics import classification_report, r2_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = ['zero', 'one', 'two', 'three', 'four',\n",
    "       'five', 'six', 'seven', 'eight', 'nine']\n",
    "\n",
    "train_test_split = 0.9\n",
    "X_train, y_train, X_test, y_test = generate_data(vals, train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['four', 'nine', 'four'],\n",
       " ['nine', 'nine', 'zero'],\n",
       " ['six', 'one', 'one'],\n",
       " ['seven', 'three', 'four'],\n",
       " ['five', 'nine', 'three']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 81, 12, 49, 60]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['eight', 'six', 'five'],\n",
       " ['six', 'nine', 'two'],\n",
       " ['two', 'one', 'zero'],\n",
       " ['nine', 'five', 'five'],\n",
       " ['two', 'five', 'five']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a feed-forward network using randomized embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 163\n",
    "num_inputs = 3\n",
    "num_layers = 2\n",
    "embed_dim = 5\n",
    "\n",
    "mod = TorchDeepNeuralEmbeddingClassifier(vals, output_size, num_inputs,\n",
    "                            num_layers, embed_dim, None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1000 of 1000; error is 0.10143007338047028"
     ]
    }
   ],
   "source": [
    "_ = mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94         8\n",
      "           1       1.00      0.00      0.00         1\n",
      "           2       1.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      1.00      0.00         0\n",
      "           5       0.00      1.00      0.00         0\n",
      "           6       1.00      1.00      1.00         1\n",
      "           7       1.00      0.75      0.86         4\n",
      "           8       1.00      0.67      0.80         3\n",
      "           9       0.00      1.00      0.00         0\n",
      "          12       0.50      1.00      0.67         1\n",
      "          13       1.00      1.00      1.00         1\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         1\n",
      "          16       1.00      0.75      0.86         4\n",
      "          17       0.00      1.00      0.00         0\n",
      "          18       1.00      0.50      0.67         2\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "          24       1.00      1.00      1.00         4\n",
      "          25       1.00      1.00      1.00         1\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      1.00      1.00         1\n",
      "          30       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         4\n",
      "          36       1.00      1.00      1.00         2\n",
      "          40       1.00      1.00      1.00         1\n",
      "          42       1.00      1.00      1.00         2\n",
      "          44       1.00      1.00      1.00         1\n",
      "          48       1.00      1.00      1.00         4\n",
      "          50       1.00      1.00      1.00         1\n",
      "          54       1.00      1.00      1.00         1\n",
      "          55       1.00      1.00      1.00         1\n",
      "          56       1.00      1.00      1.00         3\n",
      "          60       1.00      1.00      1.00         1\n",
      "          63       1.00      1.00      1.00         2\n",
      "          64       1.00      1.00      1.00         3\n",
      "          66       1.00      1.00      1.00         1\n",
      "          70       1.00      1.00      1.00         2\n",
      "          72       1.00      1.00      1.00         2\n",
      "          75       1.00      1.00      1.00         1\n",
      "          78       1.00      1.00      1.00         1\n",
      "          81       1.00      1.00      1.00         1\n",
      "          85       1.00      1.00      1.00         1\n",
      "          88       1.00      1.00      1.00         2\n",
      "          90       1.00      1.00      1.00         2\n",
      "          91       1.00      1.00      1.00         1\n",
      "         104       1.00      1.00      1.00         1\n",
      "         105       1.00      1.00      1.00         2\n",
      "         112       1.00      1.00      1.00         1\n",
      "         120       1.00      1.00      1.00         2\n",
      "         126       0.00      1.00      0.00         0\n",
      "         128       1.00      1.00      1.00         1\n",
      "         135       0.00      1.00      0.00         0\n",
      "         144       1.00      0.00      0.00         1\n",
      "         153       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.87      0.89      0.79       100\n",
      "weighted avg       0.96      0.89      0.90       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = mod.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, preds, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a feed-forward network using BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_weights_name = 'bert-base-uncased'\n",
    "# Initialize a BERT tokenizer and BERT model based on\n",
    "# `bert_weights_name`:\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "bert_model = BertModel.from_pretrained(bert_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embed = vsm.create_subword_pooling_vsm(\n",
    "    vals, bert_tokenizer, bert_model, layer=1, pool_func=vsm.mean_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>0.368413</td>\n",
       "      <td>0.714821</td>\n",
       "      <td>-0.532399</td>\n",
       "      <td>-0.153238</td>\n",
       "      <td>-0.184203</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.028352</td>\n",
       "      <td>-0.162773</td>\n",
       "      <td>0.163669</td>\n",
       "      <td>-0.471809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637450</td>\n",
       "      <td>-0.782000</td>\n",
       "      <td>0.181008</td>\n",
       "      <td>-1.160265</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>-0.899586</td>\n",
       "      <td>0.350256</td>\n",
       "      <td>-0.099035</td>\n",
       "      <td>-0.274523</td>\n",
       "      <td>0.308869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.213226</td>\n",
       "      <td>0.484864</td>\n",
       "      <td>-0.032716</td>\n",
       "      <td>-0.026842</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>-0.086201</td>\n",
       "      <td>0.195947</td>\n",
       "      <td>-0.169620</td>\n",
       "      <td>0.540456</td>\n",
       "      <td>-0.261407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455036</td>\n",
       "      <td>-0.426076</td>\n",
       "      <td>0.282586</td>\n",
       "      <td>-0.676856</td>\n",
       "      <td>-0.045337</td>\n",
       "      <td>-0.428130</td>\n",
       "      <td>0.227807</td>\n",
       "      <td>0.216206</td>\n",
       "      <td>0.112196</td>\n",
       "      <td>-0.128516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>-0.121083</td>\n",
       "      <td>0.161060</td>\n",
       "      <td>-0.549375</td>\n",
       "      <td>-0.472711</td>\n",
       "      <td>-0.146909</td>\n",
       "      <td>0.155344</td>\n",
       "      <td>-0.133190</td>\n",
       "      <td>-0.483241</td>\n",
       "      <td>0.173656</td>\n",
       "      <td>-0.328344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303860</td>\n",
       "      <td>0.224042</td>\n",
       "      <td>0.113168</td>\n",
       "      <td>-0.807975</td>\n",
       "      <td>-0.281597</td>\n",
       "      <td>-0.575383</td>\n",
       "      <td>0.138091</td>\n",
       "      <td>0.198803</td>\n",
       "      <td>-0.091571</td>\n",
       "      <td>-0.442494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>three</th>\n",
       "      <td>0.104971</td>\n",
       "      <td>0.313852</td>\n",
       "      <td>-0.340210</td>\n",
       "      <td>-0.434407</td>\n",
       "      <td>-0.049186</td>\n",
       "      <td>0.154321</td>\n",
       "      <td>-0.099751</td>\n",
       "      <td>-0.506876</td>\n",
       "      <td>0.389955</td>\n",
       "      <td>-0.244507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379173</td>\n",
       "      <td>0.134788</td>\n",
       "      <td>0.233690</td>\n",
       "      <td>-0.651362</td>\n",
       "      <td>-0.219017</td>\n",
       "      <td>-0.658988</td>\n",
       "      <td>0.203868</td>\n",
       "      <td>0.215337</td>\n",
       "      <td>-0.093137</td>\n",
       "      <td>-0.477617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>four</th>\n",
       "      <td>0.039704</td>\n",
       "      <td>0.210273</td>\n",
       "      <td>-0.526674</td>\n",
       "      <td>-0.221241</td>\n",
       "      <td>-0.102211</td>\n",
       "      <td>0.203096</td>\n",
       "      <td>-0.147871</td>\n",
       "      <td>-0.331833</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>-0.481170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240441</td>\n",
       "      <td>-0.040645</td>\n",
       "      <td>0.122894</td>\n",
       "      <td>-0.991067</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>-0.848153</td>\n",
       "      <td>0.368776</td>\n",
       "      <td>0.249214</td>\n",
       "      <td>-0.265129</td>\n",
       "      <td>-0.095733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>five</th>\n",
       "      <td>-0.039133</td>\n",
       "      <td>0.148628</td>\n",
       "      <td>-0.443105</td>\n",
       "      <td>-0.602812</td>\n",
       "      <td>0.024338</td>\n",
       "      <td>-0.077868</td>\n",
       "      <td>-0.239623</td>\n",
       "      <td>-0.467639</td>\n",
       "      <td>0.117604</td>\n",
       "      <td>-0.661229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410685</td>\n",
       "      <td>-0.126665</td>\n",
       "      <td>0.254384</td>\n",
       "      <td>-0.834398</td>\n",
       "      <td>-0.037767</td>\n",
       "      <td>-0.695847</td>\n",
       "      <td>0.188720</td>\n",
       "      <td>0.237780</td>\n",
       "      <td>-0.017827</td>\n",
       "      <td>-0.366614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>six</th>\n",
       "      <td>0.304316</td>\n",
       "      <td>0.202454</td>\n",
       "      <td>-0.506930</td>\n",
       "      <td>-0.336193</td>\n",
       "      <td>0.042346</td>\n",
       "      <td>-0.201237</td>\n",
       "      <td>-0.326339</td>\n",
       "      <td>-0.235801</td>\n",
       "      <td>0.244673</td>\n",
       "      <td>-0.600611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417288</td>\n",
       "      <td>-0.329488</td>\n",
       "      <td>0.127992</td>\n",
       "      <td>-0.932242</td>\n",
       "      <td>-0.041394</td>\n",
       "      <td>-0.668947</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>0.385106</td>\n",
       "      <td>0.048364</td>\n",
       "      <td>-0.338787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seven</th>\n",
       "      <td>0.092242</td>\n",
       "      <td>0.176112</td>\n",
       "      <td>-0.475537</td>\n",
       "      <td>-0.412275</td>\n",
       "      <td>0.071916</td>\n",
       "      <td>-0.310987</td>\n",
       "      <td>-0.048490</td>\n",
       "      <td>-0.409864</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>-0.807861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095583</td>\n",
       "      <td>-0.178779</td>\n",
       "      <td>0.421195</td>\n",
       "      <td>-0.693458</td>\n",
       "      <td>0.123755</td>\n",
       "      <td>-0.667253</td>\n",
       "      <td>0.076955</td>\n",
       "      <td>-0.138880</td>\n",
       "      <td>-0.079268</td>\n",
       "      <td>-0.244648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eight</th>\n",
       "      <td>-0.033224</td>\n",
       "      <td>0.192466</td>\n",
       "      <td>-0.507554</td>\n",
       "      <td>-0.419275</td>\n",
       "      <td>0.235861</td>\n",
       "      <td>0.105365</td>\n",
       "      <td>-0.255650</td>\n",
       "      <td>-0.200700</td>\n",
       "      <td>-0.053316</td>\n",
       "      <td>-0.733084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>-0.171980</td>\n",
       "      <td>0.286490</td>\n",
       "      <td>-0.620115</td>\n",
       "      <td>-0.104281</td>\n",
       "      <td>-0.584950</td>\n",
       "      <td>0.222393</td>\n",
       "      <td>0.159457</td>\n",
       "      <td>-0.355556</td>\n",
       "      <td>-0.570065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nine</th>\n",
       "      <td>0.105645</td>\n",
       "      <td>0.060501</td>\n",
       "      <td>-0.367164</td>\n",
       "      <td>-0.433360</td>\n",
       "      <td>0.425262</td>\n",
       "      <td>-0.032186</td>\n",
       "      <td>-0.136358</td>\n",
       "      <td>-0.358969</td>\n",
       "      <td>0.129830</td>\n",
       "      <td>-0.574778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081581</td>\n",
       "      <td>-0.280892</td>\n",
       "      <td>0.540993</td>\n",
       "      <td>-0.752147</td>\n",
       "      <td>-0.059858</td>\n",
       "      <td>-0.446215</td>\n",
       "      <td>0.033923</td>\n",
       "      <td>-0.036796</td>\n",
       "      <td>-0.296937</td>\n",
       "      <td>-0.314274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "zero   0.368413  0.714821 -0.532399 -0.153238 -0.184203  0.009934  0.028352   \n",
       "one    0.213226  0.484864 -0.032716 -0.026842  0.090642 -0.086201  0.195947   \n",
       "two   -0.121083  0.161060 -0.549375 -0.472711 -0.146909  0.155344 -0.133190   \n",
       "three  0.104971  0.313852 -0.340210 -0.434407 -0.049186  0.154321 -0.099751   \n",
       "four   0.039704  0.210273 -0.526674 -0.221241 -0.102211  0.203096 -0.147871   \n",
       "five  -0.039133  0.148628 -0.443105 -0.602812  0.024338 -0.077868 -0.239623   \n",
       "six    0.304316  0.202454 -0.506930 -0.336193  0.042346 -0.201237 -0.326339   \n",
       "seven  0.092242  0.176112 -0.475537 -0.412275  0.071916 -0.310987 -0.048490   \n",
       "eight -0.033224  0.192466 -0.507554 -0.419275  0.235861  0.105365 -0.255650   \n",
       "nine   0.105645  0.060501 -0.367164 -0.433360  0.425262 -0.032186 -0.136358   \n",
       "\n",
       "            7         8         9    ...       758       759       760  \\\n",
       "zero  -0.162773  0.163669 -0.471809  ...  0.637450 -0.782000  0.181008   \n",
       "one   -0.169620  0.540456 -0.261407  ...  0.455036 -0.426076  0.282586   \n",
       "two   -0.483241  0.173656 -0.328344  ...  0.303860  0.224042  0.113168   \n",
       "three -0.506876  0.389955 -0.244507  ...  0.379173  0.134788  0.233690   \n",
       "four  -0.331833  0.267574 -0.481170  ...  0.240441 -0.040645  0.122894   \n",
       "five  -0.467639  0.117604 -0.661229  ...  0.410685 -0.126665  0.254384   \n",
       "six   -0.235801  0.244673 -0.600611  ...  0.417288 -0.329488  0.127992   \n",
       "seven -0.409864  0.006404 -0.807861  ...  0.095583 -0.178779  0.421195   \n",
       "eight -0.200700 -0.053316 -0.733084  ...  0.233600 -0.171980  0.286490   \n",
       "nine  -0.358969  0.129830 -0.574778  ...  0.081581 -0.280892  0.540993   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "zero  -1.160265  0.005396 -0.899586  0.350256 -0.099035 -0.274523  0.308869  \n",
       "one   -0.676856 -0.045337 -0.428130  0.227807  0.216206  0.112196 -0.128516  \n",
       "two   -0.807975 -0.281597 -0.575383  0.138091  0.198803 -0.091571 -0.442494  \n",
       "three -0.651362 -0.219017 -0.658988  0.203868  0.215337 -0.093137 -0.477617  \n",
       "four  -0.991067  0.013572 -0.848153  0.368776  0.249214 -0.265129 -0.095733  \n",
       "five  -0.834398 -0.037767 -0.695847  0.188720  0.237780 -0.017827 -0.366614  \n",
       "six   -0.932242 -0.041394 -0.668947 -0.093982  0.385106  0.048364 -0.338787  \n",
       "seven -0.693458  0.123755 -0.667253  0.076955 -0.138880 -0.079268 -0.244648  \n",
       "eight -0.620115 -0.104281 -0.584950  0.222393  0.159457 -0.355556 -0.570065  \n",
       "nine  -0.752147 -0.059858 -0.446215  0.033923 -0.036796 -0.296937 -0.314274  \n",
       "\n",
       "[10 rows x 768 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 163\n",
    "num_inputs = 3\n",
    "num_layers = 2\n",
    "embed_dim = bert_embed.shape[1]\n",
    "freeze_embedding = True\n",
    "\n",
    "mod_bert_embed = TorchDeepNeuralEmbeddingClassifier(vals, output_size, num_inputs,\n",
    "                            num_layers, embed_dim, bert_embed,\n",
    "                            freeze_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1000 of 1000; error is 0.07325881719589233"
     ]
    }
   ],
   "source": [
    "_ = mod_bert_embed.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       1.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      1.00      0.00         0\n",
      "           5       0.00      1.00      0.00         0\n",
      "           6       0.50      1.00      0.67         1\n",
      "           7       1.00      0.50      0.67         4\n",
      "           8       1.00      0.33      0.50         3\n",
      "           9       0.00      1.00      0.00         0\n",
      "          12       0.25      1.00      0.40         1\n",
      "          13       1.00      0.00      0.00         1\n",
      "          14       0.00      0.00      0.00         3\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       1.00      0.75      0.86         4\n",
      "          18       0.50      0.50      0.50         2\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "          24       1.00      0.75      0.86         4\n",
      "          25       0.50      1.00      0.67         1\n",
      "          27       0.50      0.33      0.40         3\n",
      "          28       1.00      1.00      1.00         1\n",
      "          30       0.67      1.00      0.80         2\n",
      "          32       1.00      1.00      1.00         2\n",
      "          35       1.00      0.75      0.86         4\n",
      "          36       1.00      0.50      0.67         2\n",
      "          40       0.50      1.00      0.67         1\n",
      "          42       0.67      1.00      0.80         2\n",
      "          44       1.00      1.00      1.00         1\n",
      "          48       1.00      1.00      1.00         4\n",
      "          50       1.00      1.00      1.00         1\n",
      "          54       1.00      1.00      1.00         1\n",
      "          55       1.00      1.00      1.00         1\n",
      "          56       0.75      1.00      0.86         3\n",
      "          60       0.50      1.00      0.67         1\n",
      "          63       1.00      1.00      1.00         2\n",
      "          64       1.00      0.67      0.80         3\n",
      "          66       1.00      0.00      0.00         1\n",
      "          70       1.00      1.00      1.00         2\n",
      "          72       1.00      1.00      1.00         2\n",
      "          75       1.00      1.00      1.00         1\n",
      "          78       1.00      1.00      1.00         1\n",
      "          81       1.00      1.00      1.00         1\n",
      "          85       1.00      1.00      1.00         1\n",
      "          88       1.00      0.50      0.67         2\n",
      "          90       0.67      1.00      0.80         2\n",
      "          91       1.00      1.00      1.00         1\n",
      "          96       0.00      1.00      0.00         0\n",
      "          98       0.00      1.00      0.00         0\n",
      "         104       1.00      1.00      1.00         1\n",
      "         105       1.00      0.50      0.67         2\n",
      "         112       0.33      1.00      0.50         1\n",
      "         120       1.00      0.00      0.00         2\n",
      "         128       1.00      0.00      0.00         1\n",
      "         135       0.00      1.00      0.00         0\n",
      "         136       0.00      1.00      0.00         0\n",
      "         144       1.00      0.00      0.00         1\n",
      "         153       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.70       100\n",
      "   macro avg       0.71      0.73      0.58       100\n",
      "weighted avg       0.83      0.70      0.70       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = mod_bert_embed.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, preds, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a BERT model by tokenizing inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HfBertClassifierModel(nn.Module):\n",
    "    def __init__(self, n_classes, weights_name='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.weights_name = weights_name\n",
    "        self.bert = BertModel.from_pretrained(self.weights_name)\n",
    "        self.bert.train()\n",
    "        self.hidden_dim = self.bert.embeddings.word_embeddings.embedding_dim\n",
    "        # The only new parameters -- the classifier:\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.hidden_dim, self.n_classes)\n",
    "\n",
    "    def forward(self, indices, mask):\n",
    "        reps = self.bert(\n",
    "            indices, attention_mask=mask)\n",
    "        return self.classifier_layer(reps.pooler_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HfBertClassifier(TorchShallowNeuralClassifier):\n",
    "    def __init__(self, weights_name, *args, **kwargs):\n",
    "        self.weights_name = weights_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.weights_name)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.params += ['weights_name']\n",
    "\n",
    "    def build_graph(self):\n",
    "        return HfBertClassifierModel(self.n_classes_, self.weights_name)\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            X,\n",
    "            max_length=None,\n",
    "            add_special_tokens=True,\n",
    "            padding='longest',\n",
    "            return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        if y is None:\n",
    "            dataset = torch.utils.data.TensorDataset(indices, mask)\n",
    "        else:\n",
    "            self.classes_ = sorted(set(y))\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            y = [class2index[label] for label in y]\n",
    "            y = torch.tensor(y)\n",
    "            dataset = torch.utils.data.TensorDataset(indices, mask, y)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion(X):\n",
    "    new_X = []\n",
    "    for inpts in X:\n",
    "        new_X.append(' '.join(inpts))\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_bert = HfBertClassifier('bert-base-uncased', max_iter=5, batch_size=8, n_iter_no_change=5, early_stopping=True, hidden_dim=100, eta=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seven seven four',\n",
       " 'seven nine nine',\n",
       " 'four eight five',\n",
       " 'eight nine two',\n",
       " 'one five zero']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bert = conversion(X_train)\n",
    "X_test_bert = conversion(X_test)\n",
    "X_train_bert[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 250.60060513019562"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HfBertClassifier(\n",
       "\tbatch_size=8,\n",
       "\tmax_iter=5,\n",
       "\teta=5e-05,\n",
       "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tgradient_accumulation_steps=1,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=True,\n",
       "\tn_iter_no_change=5,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05,\n",
       "\thidden_dim=100,\n",
       "\thidden_activation=Tanh(),\n",
       "\tweights_name=bert-base-uncased)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_bert.fit(X_train_bert, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93         8\n",
      "           1       0.00      0.00      0.00         1\n",
      "           4       0.50      0.50      0.50         2\n",
      "           6       0.50      1.00      0.67         1\n",
      "           8       0.00      0.00      0.00         5\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         2\n",
      "          12       0.50      0.20      0.29         5\n",
      "          15       0.00      0.00      0.00         1\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.07      1.00      0.13         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         3\n",
      "          24       0.25      0.60      0.35         5\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.00      0.00      0.00         2\n",
      "          28       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00         6\n",
      "          32       0.00      0.00      0.00         2\n",
      "          36       0.29      1.00      0.44         2\n",
      "          40       0.50      1.00      0.67         3\n",
      "          42       1.00      0.50      0.67         4\n",
      "          44       0.00      0.00      0.00         2\n",
      "          45       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         3\n",
      "          49       0.00      0.00      0.00         0\n",
      "          51       0.00      0.00      0.00         1\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         2\n",
      "          56       0.00      0.00      0.00         3\n",
      "          60       0.00      0.00      0.00         2\n",
      "          63       0.33      1.00      0.50         1\n",
      "          64       0.00      0.00      0.00         1\n",
      "          70       0.00      0.00      0.00         1\n",
      "          72       0.56      0.83      0.67         6\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          81       0.00      0.00      0.00         2\n",
      "          84       0.00      0.00      0.00         2\n",
      "          90       0.20      1.00      0.33         1\n",
      "          91       0.00      0.00      0.00         0\n",
      "          96       1.00      1.00      1.00         1\n",
      "          98       0.00      0.00      0.00         2\n",
      "          99       0.00      0.00      0.00         1\n",
      "         112       0.00      0.00      0.00         3\n",
      "         126       0.00      0.00      0.00         0\n",
      "         135       0.00      0.00      0.00         1\n",
      "         144       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.29       100\n",
      "   macro avg       0.14      0.21      0.15       100\n",
      "weighted avg       0.24      0.29      0.24       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = mod_bert.predict(X_test_bert)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IIT Dataset that computes y + z then x * (x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = 1\n",
    "train_data_V1, test_data_V1 = get_iit_distribution_dataset_both(V1, vals, 0.9, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_base_train_V1, y_base_train_V1, x_source_train_V1, y_source_train_V1 = train_data_V1\n",
    "x_base_test_V1, y_base_test_V1, x_source_test_V1, y_source_test_V1 = test_data_V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['one', 'six', 'zero'], ['five', 'nine', 'zero'], ['seven', 'nine', 'six']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_base_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['five', 'two', 'four'],\n",
       " ['three', 'seven', 'six'],\n",
       " ['eight', 'eight', 'three']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_source_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 65, 77]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_source_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(x_base_train_V1)\n",
    "interventions_train_V1 = torch.zeros(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = len(x_base_test_V1)\n",
    "interventions_test_V1 = torch.zeros(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIT on Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = 0\n",
    "coords = {\n",
    "    V1: [{\"layer\": 1, \"start\": 0, \"end\": 15}], \n",
    "}\n",
    "\n",
    "V1_model = TorchLinearEmbeddingRegressionIIT(\n",
    "    hidden_dim=50, \n",
    "    hidden_activation=torch.nn.ReLU(), \n",
    "    num_layers=3, \n",
    "    id_to_coords=coords, \n",
    "    embed_dim=5,\n",
    "    vocab=vals,\n",
    "    num_inputs=3,\n",
    "    max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 2000 of 2000; error is 5.2497859001159675"
     ]
    }
   ],
   "source": [
    "_ = V1_model.fit(\n",
    "    x_base_train_V1, \n",
    "    [x_source_train_V1], \n",
    "    y_base_train_V1, \n",
    "    y_source_train_V1, \n",
    "    interventions_train_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 85., 17., 58.,  0.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IIT_preds_V1, base_preds_V1 = V1_model.iit_predict(\n",
    "    x_base_test_V1, [x_source_test_V1], interventions_test_V1\n",
    ")\n",
    "#y_base_test_V1[:5]\n",
    "base_preds_V1.detach()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Evaluation\n",
      "0.9968243746800389\n",
      "V1 counterfactual evaluation\n",
      "0.9888175325568104\n"
     ]
    }
   ],
   "source": [
    "print('Standard Evaluation')\n",
    "print(r2_score(y_base_test_V1, base_preds_V1.detach()))\n",
    "      \n",
    "print(\"V1 counterfactual evaluation\")\n",
    "print(r2_score(y_source_test_V1, IIT_preds_V1.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IIT Dataset that computes xy and yz then xy + yz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = 1\n",
    "V2 = 2\n",
    "\n",
    "train_data_V1, test_data_V1 = get_iit_distribution_dataset_both(V1, vals)\n",
    "train_data_V2, test_data_V2 = get_iit_distribution_dataset_both(V2, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_base_train_V1, y_base_train_V1, x_source_train_V1, y_source_train_V1 = train_data_V1\n",
    "x_base_test_V1, y_base_test_V1, x_source_test_V1, y_source_test_V1 = test_data_V1\n",
    "\n",
    "x_base_train_V2, y_base_train_V2, x_source_train_V2, y_source_train_V2 = train_data_V2\n",
    "x_base_test_V2, y_base_test_V2, x_source_test_V2, y_source_test_V2 = test_data_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(x_base_train_V1)\n",
    "interventions_train_V1 = torch.zeros(train_size)\n",
    "interventions_train_V2 = torch.ones(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = len(x_base_test_V1)\n",
    "interventions_test_V1 = torch.zeros(test_size)\n",
    "interventions_test_V2 = torch.ones(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nine', 'three', 'zero'], ['six', 'four', 'zero'], ['four', 'two', 'three']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_base_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['four', 'nine', 'zero'], ['nine', 'six', 'zero'], ['five', 'two', 'three']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_source_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 54, 22]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_source_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base_train_both = np.concatenate([np.array(x_base_train_V1),\n",
    "                                np.array(x_base_train_V2)], axis=0)\n",
    "\n",
    "X_sources_train_both = [np.concatenate([np.array(x_source_train_V1),\n",
    "                            np.array(x_source_train_V2)], axis=0)]\n",
    "\n",
    "y_base_train_both = np.concatenate([np.array(y_base_train_V1),\n",
    "                            np.array(y_base_train_V2)], axis=0)\n",
    "\n",
    "y_source_train_both = np.concatenate([np.array(y_source_train_V1),\n",
    "                            np.array(y_source_train_V2)])\n",
    "\n",
    "interventions_train_both = np.concatenate([np.array(interventions_train_V1),\n",
    "                            np.array(interventions_train_V2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base_test_both = np.concatenate([np.array(x_base_test_V1),\n",
    "                                np.array(x_base_test_V2)], axis=0)\n",
    "\n",
    "X_sources_test_both = [np.concatenate([np.array(x_source_test_V1),\n",
    "                            np.array(x_source_test_V2)], axis=0)]\n",
    "\n",
    "y_base_test_both = np.concatenate([np.array(y_base_test_V1),\n",
    "                            np.array(y_base_test_V2)], axis=0)\n",
    "\n",
    "y_source_test_both = np.concatenate([np.array(y_source_test_V1),\n",
    "                            np.array(y_source_test_V2)])\n",
    "\n",
    "interventions_test_both = np.concatenate([np.array(interventions_test_V1),\n",
    "                            np.array(interventions_test_V2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model without IIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterventionableTorchLinearEmbeddingRegression(TorchLinearEmbeddingRegression):\n",
    "    def __init__(self, **base_kwargs):\n",
    "        super().__init__(**base_kwargs)\n",
    "        \n",
    "    def make_hook(self, gets, sets, layer):\n",
    "        def hook(model, input, output):\n",
    "            layer_gets, layer_sets = [], []\n",
    "            if gets is not None and layer in gets:\n",
    "                layer_gets = gets[layer]\n",
    "            if sets is not None and layer in sets:\n",
    "                layer_sets = sets[layer]\n",
    "            for set in layer_sets:\n",
    "                output = torch.cat(\n",
    "                    [output[:, :set[\"start\"]], \n",
    "                     set[\"intervention\"], \n",
    "                     output[:, set[\"end\"]: ]], \n",
    "                    dim=1)\n",
    "            for get in layer_gets:\n",
    "                k = f'{get[\"layer\"]}-{get[\"start\"]}-{get[\"end\"]}'\n",
    "                self.activation[k] = output[:, get[\"start\"]: get[\"end\"] ]\n",
    "            return output\n",
    "        return hook\n",
    "\n",
    "    def _gets_sets(self, gets=None, sets=None):\n",
    "        handlers = []\n",
    "        for layer in range(len(self.layers)):\n",
    "            hook = self.make_hook(gets, sets, layer)\n",
    "            both_handler = self.layers[layer].register_forward_hook(hook)\n",
    "            handlers.append(both_handler)\n",
    "        return handlers\n",
    "\n",
    "    def retrieve_activations(self, X, get, sets):\n",
    "        if sets is not None and \"intervention\" in sets:\n",
    "            sets[\"intervention\"] = sets[\"intervention\"].type(torch.FloatTensor).to(self.device)\n",
    "        X = X.type(torch.LongTensor).to(self.device)\n",
    "        self.activation = {}\n",
    "        get_val = {get[\"layer\"]: [get]} if get is not None else None\n",
    "        set_val = {sets[\"layer\"]: [sets]} if sets is not None else None\n",
    "        handlers = self._gets_sets(get_val, set_val)\n",
    "        logits = self.model(X)\n",
    "        for handler in handlers:\n",
    "            handler.remove()\n",
    "        return self.activation[f'{get[\"layer\"]}-{get[\"start\"]}-{get[\"end\"]}']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regression = InterventionableTorchLinearEmbeddingRegression(vocab=vals, num_inputs=3,\n",
    "                                                 num_layers=2, hidden_dim=50,\n",
    "                                                 embed_dim=5, max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 690 of 2000; error is 922.40423583984388"
     ]
    }
   ],
   "source": [
    "_ = model_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4435920949559655"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_regression.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4950804790690374"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causual Abstraction Analysis for Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 5\n",
    "\n",
    "alignment = {\n",
    "    \"V1\": {\"layer\": 1, \"start\": 0, \"end\": embedding_dim*3}, \n",
    "    \"V2\": {\"layer\": 1, \"start\": embedding_dim*3, \"end\": embedding_dim*6}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interchange_intervention(model, base, source, get_coord, output_coord):\n",
    "    intervention = model.retrieve_activations(source, get_coord, None)\n",
    "    get_coord[\"intervention\"] = intervention\n",
    "    return model.retrieve_activations(base, output_coord, get_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_coord = {\"layer\": 2, \"start\": 0, \"end\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ActivationLayer(\n",
       "   (linear): Linear(in_features=15, out_features=50, bias=True)\n",
       "   (activation): Tanh()\n",
       " ),\n",
       " ActivationLayer(\n",
       "   (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "   (activation): Tanh()\n",
       " )]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_regression.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ii_evaluation(X_base, X_source, y_base, y_source, model,\n",
    "                variable, output_coord):\n",
    "    predictions = []\n",
    "    for base, source in zip(X_base, X_source):\n",
    "        base = base.unsqueeze(0)\n",
    "        source = source.unsqueeze(0)\n",
    "        network_output = interchange_intervention(\n",
    "            model,\n",
    "            base,\n",
    "            source,\n",
    "            alignment[variable],\n",
    "            output_coord\n",
    "        )\n",
    "        pred = network_output\n",
    "        predictions.append(torch.round(pred).detach().squeeze(1))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input(X):\n",
    "    new_X = []\n",
    "    index = dict(zip(vals, range(len(vals))))\n",
    "    for ex in X:\n",
    "        seq = [index[w] for w in ex]\n",
    "        seq = torch.tensor(seq)\n",
    "        new_X.append(seq)\n",
    "    X = torch.stack(new_X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0-0-15': tensor([[-0.8210,  0.9014, -0.7279, -0.6702, -0.6303,  0.8824, -0.6798, -0.5829,\n",
      "          0.6721,  0.8812,  0.7270,  0.9306,  0.9051, -0.9596,  0.3632]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.8579]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.7822,  0.4384, -0.6797, -0.7883, -0.9060,  0.7664,  0.2630,  0.5538,\n",
      "          0.7599,  0.5879, -0.1031,  0.8374,  0.7669, -0.6954, -0.1511]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[13.8107]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.1353,  0.9072, -0.8552, -0.7492, -0.7483,  0.6671,  0.0758,  0.1312,\n",
      "          0.1514,  0.9441,  0.5104,  0.7712,  0.8021, -0.7878, -0.8646]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[14.3852]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9525,  0.9455, -0.9672, -0.9557, -0.9912,  0.9905, -0.8160,  0.0022,\n",
      "         -0.9696,  0.1077,  0.9612,  0.9684,  0.9680, -0.9971, -0.7048]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[7.1640]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.1984,  0.7852, -0.8199, -0.7025, -0.7907,  0.5015, -0.9617, -0.9065,\n",
      "         -0.4495, -0.2390,  0.9466,  0.6592,  0.6666, -0.9183, -0.4556]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.4051]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.7068,  0.6303, -0.7788, -0.8901, -0.8727,  0.5932, -0.5869, -0.5165,\n",
      "          0.6628,  0.9270,  0.6179,  0.5631,  0.7897, -0.8714,  0.8462]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.8612]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.9236,  0.6680, -0.5983, -0.7333, -0.8957,  0.6012, -0.5793, -0.3937,\n",
      "          0.8944,  0.8322,  0.6064,  0.7284,  0.7988, -0.8914,  0.8638]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[44.4100]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.1922,  0.9245, -0.9499, -0.8720, -0.9397,  0.7013, -0.9346, -0.8751,\n",
      "          0.1941,  0.6158,  0.9071,  0.6868,  0.7914, -0.9731,  0.6528]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[27.9586]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.1309,  0.9760, -0.2061, -0.5168, -0.8969,  0.9102, -0.1018, -0.0491,\n",
      "         -0.4913,  0.9846,  0.5047,  0.7127,  0.8332, -0.8722,  0.7375]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.8960]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.3073, -0.1879, -0.8417, -0.7680, -0.7815,  0.3433, -0.6232, -0.4692,\n",
      "          0.6646,  0.2882,  0.7800,  0.6718, -0.7727, -0.5192, -0.4320]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.7011]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9583,  0.7430, -0.6765, -0.6824, -0.7204,  0.6076, -0.7205, -0.2923,\n",
      "         -0.8037, -0.7927,  0.7112,  0.4260,  0.9019, -0.7306, -0.0887]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[20.0446]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9652,  0.9509, -0.6173, -0.3601, -0.9816,  0.9809,  0.4188,  0.5213,\n",
      "         -0.9925, -0.6876,  0.6786,  0.8729,  0.3969, -0.9930,  0.1532]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[40.3975]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.8955,  0.9237, -0.3084, -0.5945, -0.9058,  0.8868,  0.1510,  0.3054,\n",
      "         -0.9761, -0.6742,  0.2173,  0.3432, -0.2434, -0.9615,  0.7847]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[0.4655]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.5624,  0.8007, -0.8304, -0.7939, -0.6743,  0.8761, -0.7081, -0.6829,\n",
      "          0.4123,  0.9010,  0.6669,  0.8699,  0.8681, -0.9352,  0.5640]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.5233]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.7510,  0.9921, -0.8971, -0.5345, -0.9518,  0.8932, -0.8462, -0.7312,\n",
      "         -0.8516,  0.8866,  0.9147,  0.8561,  0.4629, -0.9881,  0.0861]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[42.5608]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.2372,  0.5027, -0.9769, -0.2082, -0.9155,  0.7396, -0.9783, -0.9853,\n",
      "         -0.4460, -0.3555,  0.9640,  0.7864,  0.5131, -0.9231, -0.5649]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[10.1452]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.3569,  0.6566, -0.8902, -0.7679, -0.7918,  0.5858, -0.8425, -0.6771,\n",
      "         -0.7148, -0.6633,  0.8237,  0.5938,  0.2585, -0.7704, -0.9269]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[31.7349]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.6193,  0.8853, -0.7412, -0.4657, -0.8071,  0.9470, -0.8969, -0.8079,\n",
      "         -0.7222,  0.6371,  0.9225,  0.9632, -0.5591, -0.9717,  0.1355]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[43.1240]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.3843,  0.7616, -0.5077, -0.6466, -0.6624,  0.8841, -0.9017, -0.7948,\n",
      "          0.4776,  0.6277,  0.9478,  0.8934,  0.8187, -0.7778,  0.1393]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[28.8630]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.2743,  0.8221, -0.7274, -0.7386, -0.8214,  0.9321, -0.9464, -0.8699,\n",
      "         -0.1265,  0.5047,  0.9499,  0.9042,  0.9181, -0.8167,  0.0646]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.7738]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.6024,  0.8400, -0.9278, -0.7364, -0.7289,  0.4688, -0.9451, -0.9181,\n",
      "         -0.3014,  0.9473,  0.9751,  0.8287,  0.4515, -0.8922, -0.7006]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[31.7991]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.3773,  0.5988, -0.6385, -0.4556, -0.6864,  0.7130, -0.8444, -0.8590,\n",
      "          0.2029,  0.6384,  0.9512,  0.8579, -0.7184, -0.9280,  0.4545]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[17.3779]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.7146,  0.8080, -0.9712, -0.7726, -0.8275,  0.5010, -0.9633, -0.9097,\n",
      "         -0.8418, -0.8934,  0.9488,  0.4635, -0.3182, -0.9015, -0.9430]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[40.1717]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9825,  0.8422, -0.9471, -0.8064, -0.9416,  0.8967, -0.2446,  0.0882,\n",
      "         -0.9804, -0.9029,  0.8330,  0.6917,  0.9642, -0.9891, -0.6744]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[43.6998]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.4254,  0.9461, -0.6090, -0.8182, -0.9617,  0.9553, -0.6932, -0.2401,\n",
      "         -0.8922,  0.3595,  0.7721,  0.9280,  0.7409, -0.9821,  0.2910]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[23.9444]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.0252,  0.3953, -0.9466, -0.8019, -0.7638,  0.6787, -0.9574, -0.9669,\n",
      "          0.1149,  0.5854,  0.9634,  0.8133,  0.3173, -0.8459, -0.2174]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[25.6638]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.3509,  0.2982, -0.4853, -0.6757, -0.8021,  0.7641, -0.5301, -0.1677,\n",
      "          0.6865,  0.2691,  0.6355,  0.7399,  0.4020,  0.0179, -0.2802]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[14.9847]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.6342,  0.5818, -0.7976, -0.7806, -0.4281,  0.8332,  0.1684,  0.0973,\n",
      "          0.5866,  0.8021,  0.1532,  0.8238,  0.3461, -0.7758, -0.4056]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.8017]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.8075,  0.9749, -0.8232, -0.9106, -0.9251,  0.9326, -0.6642,  0.0817,\n",
      "         -0.7217,  0.9676,  0.7539,  0.8120,  0.9743, -0.9431, -0.1160]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.5943]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9873,  0.7639, -0.9768, -0.9434, -0.9303,  0.9364, -0.9848, -0.5255,\n",
      "         -0.9817, -0.9018,  0.9565,  0.8632,  0.8308, -0.9642, -0.8780]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[27.1585]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-4.4587e-01,  6.9271e-01, -8.9953e-01, -9.2540e-01, -8.3066e-01,\n",
      "          6.7409e-01, -8.3773e-01, -8.6992e-01,  7.3758e-04,  6.0705e-01,\n",
      "          8.0239e-01,  3.3042e-01,  9.2121e-01, -8.0519e-01,  4.7703e-01]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[25.8794]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.6622,  0.8914, -0.6452, -0.6093, -0.9281,  0.8721, -0.9019, -0.7510,\n",
      "         -0.9373, -0.3474,  0.8445,  0.6103,  0.1775, -0.9312,  0.3470]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.8352]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9766,  0.6740, -0.9311, -0.5961, -0.3165,  0.6250, -0.8508, -0.7181,\n",
      "         -0.9067, -0.8388,  0.8532,  0.7674,  0.2766, -0.9547, -0.6999]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[12.6607]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.7029,  0.3827, -0.7882,  0.1042, -0.7725,  0.9199, -0.9082, -0.8049,\n",
      "         -0.5348, -0.7847,  0.9362,  0.8770,  0.3289, -0.2584, -0.9518]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[35.9749]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.8678,  0.7887, -0.9511, -0.9673, -0.9522,  0.9533, -0.7953,  0.4774,\n",
      "         -0.5945,  0.2931,  0.7373,  0.9018,  0.9262, -0.9644,  0.1086]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[18.8918]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.5102,  0.7226, -0.7094, -0.8061, -0.8118,  0.7838, -0.4762, -0.6076,\n",
      "          0.3208,  0.6777,  0.9407,  0.2983,  0.9130, -0.9370,  0.7549]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.7488]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.1985,  0.6170, -0.9483, -0.8115, -0.5681,  0.9220, -0.7898, -0.6726,\n",
      "          0.4320,  0.6133,  0.6511,  0.8941,  0.9700, -0.7909, -0.0018]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[20.3172]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.3729,  0.7503, -0.9488, -0.7187, -0.6781,  0.2995, -0.9427, -0.9601,\n",
      "         -0.0282,  0.9561,  0.9675,  0.7452,  0.5988, -0.7325, -0.7529]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[19.4620]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.6433,  0.9569, -0.3623, -0.3312, -0.8064,  0.7737, -0.0844, -0.0611,\n",
      "          0.2517,  0.9824,  0.6390,  0.9067,  0.9048, -0.9245,  0.0417]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.8997]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.6924,  0.6333, -0.8540, -0.6381, -0.7565,  0.6767, -0.9943, -0.9882,\n",
      "         -0.7769, -0.6016,  0.9824,  0.6402,  0.8628, -0.2882, -0.8110]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[13.2956]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.2790,  0.5820, -0.8321, -0.9258, -0.7306,  0.6950, -0.1802, -0.0302,\n",
      "         -0.0480,  0.2324,  0.5287,  0.3808,  0.4037, -0.7388, -0.4126]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[26.0948]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.3289,  0.9266, -0.8899, -0.7426, -0.8567,  0.7456, -0.6426, -0.6720,\n",
      "          0.1654,  0.9791,  0.7179,  0.8765,  0.9915, -0.8388, -0.5847]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.8603]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9715,  0.5985, -0.9139, -0.0279, -0.8184,  0.7609, -0.7191, -0.5440,\n",
      "         -0.9491, -0.9770,  0.6409,  0.2743,  0.8620, -0.9158, -0.3838]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[6.6656]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.7524,  0.7142, -0.4652, -0.8699, -0.5062,  0.3973, -0.7284, -0.6757,\n",
      "          0.5456,  0.6161,  0.8438,  0.1254,  0.8875, -0.8417,  0.8003]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[41.8647]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.7124,  0.0815, -0.7446, -0.7025, -0.7803,  0.2335, -0.8992, -0.8315,\n",
      "          0.8379,  0.6920,  0.9322,  0.7268, -0.4517, -0.8117,  0.5929]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.2384]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.7653,  0.7330, -0.8049, -0.8919, -0.8826,  0.6004, -0.7829, -0.6924,\n",
      "          0.5431,  0.8219,  0.4886,  0.7762,  0.9749, -0.7692,  0.3448]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.9026]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.1182,  0.4909, -0.8909, -0.9196, -0.6503,  0.7420, -0.8259, -0.4249,\n",
      "          0.1104,  0.0471,  0.8974,  0.6987,  0.7648, -0.8887,  0.0382]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[32.4190]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.5266,  0.8361, -0.8308, -0.7417, -0.6322,  0.9063, -0.0904,  0.0170,\n",
      "          0.3813,  0.6783,  0.2844,  0.9146,  0.7441, -0.8820, -0.6434]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[30.3618]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9276,  0.7831, -0.7577, -0.8699, -0.6835,  0.5329, -0.4376, -0.2008,\n",
      "         -0.9218, -0.8471,  0.4745, -0.2847,  0.9076, -0.8794,  0.3294]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[15.6490]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.7737,  0.9417, -0.8064, -0.6438, -0.8823,  0.9346, -0.4066,  0.2362,\n",
      "         -0.7257,  0.8989,  0.7026,  0.8864,  0.7852, -0.9251,  0.0193]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[25.2139]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.0497,  0.9624, -0.9624, -0.7619, -0.8366,  0.9131, -0.9566, -0.9242,\n",
      "         -0.1614,  0.5075,  0.9208,  0.9117,  0.8692, -0.9868,  0.1746]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.7894]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.6026,  0.9021, -0.0804, -0.1346, -0.9659,  0.8773,  0.3893,  0.5465,\n",
      "         -0.8109, -0.3927, -0.2858,  0.6493, -0.1208, -0.9553,  0.9302]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.3396]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9221,  0.8178, -0.9029, -0.7078, -0.9697,  0.9536, -0.8950, -0.8515,\n",
      "         -0.9569,  0.1044,  0.9907,  0.8911, -0.7409, -0.9881, -0.1248]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[34.4200]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.0629,  0.3742, -0.8698, -0.8571, -0.4171, -0.0070, -0.9712, -0.9445,\n",
      "         -0.0113, -0.0072,  0.9880,  0.5980, -0.2685, -0.9337, -0.2512]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.6334]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9369,  0.8802, -0.9572, -0.8984, -0.8581,  0.9604, -0.9734, -0.7573,\n",
      "         -0.8580,  0.3617,  0.9662,  0.9771,  0.3275, -0.9878, -0.7283]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[13.1575]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.8001,  0.8251, -0.7197, -0.7346, -0.6138,  0.8932, -0.6300, -0.8024,\n",
      "          0.6880,  0.9323,  0.4305,  0.7304,  0.9772, -0.6602,  0.7277]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.6338]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.5443,  0.8223, -0.8662, -0.9711, -0.9394,  0.8112, -0.8368, -0.0557,\n",
      "         -0.5317,  0.6838,  0.6771,  0.7572,  0.7262, -0.9621,  0.4747]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[12.3956]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.8597,  0.8597, -0.9347, -0.9765, -0.9465,  0.9111, -0.9023, -0.3708,\n",
      "         -0.9053,  0.2415,  0.9035,  0.8016,  0.7002, -0.9850, -0.1117]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[17.8198]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.8461,  0.7408, -0.6849, -0.8025, -0.7334,  0.8078, -0.4686, -0.4118,\n",
      "          0.7510,  0.8531,  0.6960,  0.7137,  0.8355, -0.9126,  0.7871]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[29.0000]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.8762,  0.3025, -0.3787, -0.7879, -0.8219,  0.6557,  0.4278,  0.4201,\n",
      "          0.8905,  0.7709, -0.2687,  0.2448,  0.5773, -0.1148,  0.7132]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[42.3398]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9750,  0.8886, -0.9812, -0.9288, -0.9600,  0.8227, -0.9935, -0.9101,\n",
      "         -0.9859, -0.7811,  0.9834,  0.8484,  0.1531, -0.9921, -0.8787]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[14.6715]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.2506,  0.6313, -0.8635, -0.7150, -0.8956,  0.9399, -0.3652, -0.2002,\n",
      "         -0.4158,  0.1493,  0.9365,  0.7736,  0.4918, -0.9573,  0.4200]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[41.4116]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.8779,  0.7028, -0.8704,  0.0533, -0.9460,  0.9795, -0.8758, -0.5129,\n",
      "         -0.9014, -0.7623,  0.8712,  0.9234,  0.4013, -0.9369, -0.0686]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[24.5268]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.4406,  0.8201, -0.8836, -0.7772, -0.9031,  0.7731, -0.9699, -0.8853,\n",
      "         -0.8228, -0.3726,  0.9277,  0.8462,  0.9337, -0.9317, -0.7080]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[44.5450]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.8215,  0.6198, -0.7978, -0.9090, -0.8097,  0.3923, -0.7350, -0.7612,\n",
      "          0.7207,  0.8602,  0.4123,  0.2258,  0.9289, -0.5683,  0.8169]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.9057]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.5503,  0.2588, -0.7421, -0.8271, -0.3629, -0.3891, -0.9507, -0.8946,\n",
      "          0.7149,  0.5245,  0.9552,  0.5202, -0.2187, -0.8392,  0.3554]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.8978]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.1615,  0.3937, -0.7915,  0.2748, -0.8043,  0.9153, -0.8185, -0.6379,\n",
      "         -0.2509, -0.6171,  0.8780,  0.8526,  0.3542, -0.8470,  0.0739]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[6.1416]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.3167,  0.5974, -0.7589, -0.7115, -0.5191,  0.0985, -0.9404, -0.8800,\n",
      "         -0.0983, -0.3525,  0.9242,  0.1528,  0.7580, -0.8573,  0.1152]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[27.9803]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9753,  0.8377, -0.6730, -0.6560, -0.8747,  0.9252, -0.1023,  0.5197,\n",
      "         -0.9856, -0.9159,  0.4313,  0.6129, -0.2059, -0.9541,  0.3712]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[44.8461]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9742,  0.8657, -0.7485, -0.4566, -0.3652,  0.8813, -0.8073, -0.5109,\n",
      "         -0.8993, -0.8448,  0.7501,  0.8179,  0.9399, -0.8590, -0.5997]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[23.7445]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9340,  0.9169, -0.9780, -0.9749, -0.9647,  0.8924, -0.9716, -0.7372,\n",
      "         -0.9556, -0.1763,  0.9695,  0.8807,  0.3746, -0.9958, -0.5678]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[40.0614]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.8456,  0.8075, -0.8080, -0.7929, -0.8639,  0.6112, -0.0489,  0.4559,\n",
      "         -0.7417, -0.5917,  0.0357,  0.6151,  0.9686, -0.9474,  0.0467]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[11.2526]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.7140,  0.9228, -0.7108, -0.6872, -0.8553,  0.4161, -0.8686, -0.8202,\n",
      "          0.3826,  0.5180,  0.8383,  0.5425, -0.1010, -0.9556,  0.8505]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.9053]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.1179,  0.4366, -0.9542, -0.8734, -0.7812,  0.8893, -0.7843, -0.6618,\n",
      "          0.3426,  0.4094,  0.7562,  0.7950,  0.9540, -0.7866,  0.2065]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[9.9314]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9363,  0.8642, -0.9691, -0.9639, -0.9827,  0.9742, -0.7921, -0.2667,\n",
      "         -0.9447,  0.3386,  0.9641,  0.8577,  0.7627, -0.9943, -0.2234]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[34.2322]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.8157,  0.6744, -0.8200, -0.8773, -0.7604,  0.0082, -0.8686, -0.7874,\n",
      "          0.7096,  0.7896,  0.8561,  0.6020,  0.0245, -0.9370,  0.5808]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[17.9588]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9797,  0.6894, -0.9016, -0.7115, -0.6927,  0.2660, -0.8578, -0.6399,\n",
      "         -0.9806, -0.9813,  0.7158, -0.2588,  0.7987, -0.8915, -0.5658]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[16.9734]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.2148,  0.2264, -0.9092, -0.8029, -0.6321,  0.6994, -0.7079, -0.6292,\n",
      "          0.0665,  0.2004,  0.9004,  0.8319, -0.6859, -0.7918, -0.8270]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[12.3654]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.3487,  0.4747, -0.7850, -0.7464, -0.6560,  0.2387, -0.9363, -0.8910,\n",
      "         -0.2100,  0.0349,  0.9302,  0.3844,  0.2031, -0.8420, -0.1568]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.9001]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.6487,  0.7251, -0.9175, -0.7902, -0.9255,  0.9386, -0.9446, -0.9530,\n",
      "         -0.6256,  0.2736,  0.9882,  0.7890,  0.9327, -0.7992, -0.4773]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[10.0326]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.4408,  0.8704, -0.9588, -0.8966, -0.8936,  0.3596, -0.9644, -0.9572,\n",
      "          0.3029,  0.4532,  0.8769,  0.4294,  0.8116, -0.9036,  0.4982]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[32.6892]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.9384,  0.7155, -0.2156, -0.8140, -0.7518,  0.5920, -0.2332, -0.2201,\n",
      "          0.9167,  0.9581,  0.4541,  0.3798,  0.8737, -0.6777,  0.9372]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[30.8945]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.0822,  0.0869, -0.8444, -0.8815, -0.5861,  0.0790, -0.7702, -0.5933,\n",
      "          0.3493,  0.3073,  0.8230,  0.6012, -0.7719, -0.5152, -0.7204]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[28.3357]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.7287,  0.8379, -0.8813, -0.7025, -0.5827,  0.7524, -0.8348, -0.8149,\n",
      "          0.6003,  0.8441,  0.8802,  0.9142,  0.4030, -0.9772,  0.1800]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[35.5802]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.1282,  0.5933, -0.9181, -0.8877, -0.9060,  0.9038,  0.2215,  0.0856,\n",
      "         -0.3164,  0.3301,  0.7999,  0.5245,  0.5102, -0.8924, -0.5034]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[15.1518]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.3159,  0.7871, -0.9033, -0.9112, -0.8959,  0.7996, -0.8685, -0.8294,\n",
      "         -0.2911,  0.5175,  0.8340,  0.8175,  0.9722, -0.9027, -0.2625]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.8843]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9946,  0.5499, -0.9793, -0.8639, -0.8717,  0.8839, -0.3724,  0.1996,\n",
      "         -0.9786, -0.9705,  0.8723,  0.3810,  0.9068, -0.9727, -0.6034]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[4.1856]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.8491,  0.8290, -0.8481, -0.7973, -0.8503,  0.4732, -0.2372,  0.1351,\n",
      "         -0.7258, -0.6135,  0.2095,  0.3115,  0.9234, -0.9488,  0.3476]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[20.5958]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9707,  0.9196, -0.9897, -0.9616, -0.9887,  0.9686, -0.9370, -0.6796,\n",
      "         -0.9744, -0.0718,  0.9889,  0.9155,  0.4844, -0.9984, -0.6407]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[40.5000]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.5534,  0.7588, -0.6484, -0.4770, -0.4467,  0.9314, -0.3737, -0.0622,\n",
      "          0.5025,  0.6634,  0.5583,  0.9202,  0.6959, -0.9007,  0.6103]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[24.7886]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.2814,  0.9550, -0.5842, -0.5900, -0.8975,  0.8484, -0.2185, -0.1475,\n",
      "         -0.5788,  0.9810,  0.6449,  0.7910,  0.4593, -0.9413,  0.4994]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.7727]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.7401,  0.1109, -0.9438, -0.8321, -0.6142,  0.6674, -0.8269, -0.3396,\n",
      "         -0.7089, -0.8805,  0.8131,  0.5569,  0.2243, -0.4968, -0.9500]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[8.7506]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.1468,  0.6588, -0.8942, -0.2604, -0.9099,  0.9005, -0.9213, -0.8849,\n",
      "         -0.3588, -0.1534,  0.8868,  0.7543,  0.8762, -0.8919,  0.1407]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[12.5416]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.8948,  0.9013, -0.7570, -0.9427, -0.9318,  0.9260, -0.9726, -0.5178,\n",
      "         -0.7829,  0.1909,  0.9273,  0.9067,  0.8832, -0.8924, -0.2628]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[24.2176]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.8370,  0.7797, -0.6252, -0.6297, -0.8595,  0.4214, -0.5431, -0.6314,\n",
      "          0.6523,  0.8834,  0.7753,  0.6858, -0.4068, -0.9547,  0.8644]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.6113]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.2741,  0.9189, -0.8730, -0.3901, -0.8555,  0.5211, -0.9817, -0.9690,\n",
      "         -0.7962, -0.7096,  0.9752,  0.5318, -0.3747, -0.9672, -0.0955]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[3.0053]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.7472,  0.8860, -0.7066, -0.3803, -0.6445,  0.8095, -0.6746, -0.7644,\n",
      "          0.3968,  0.8432,  0.8065,  0.9113, -0.1752, -0.9777,  0.6085]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.8958]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[-0.7042,  0.9065, -0.6965, -0.7195, -0.6389,  0.5392, -0.3417, -0.3296,\n",
      "          0.6813,  0.9865,  0.7260,  0.6170,  0.9168, -0.8441,  0.2960]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[45.7639]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.9320,  0.9864, -0.7635, -0.0223, -0.8901,  0.9591, -0.4748, -0.3270,\n",
      "         -0.9698, -0.7009,  0.5199,  0.8692, -0.3014, -0.9935,  0.5395]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[22.1451]], grad_fn=<SliceBackward0>)}\n",
      "{'0-0-15': tensor([[ 0.6310,  0.8496, -0.0688, -0.4170, -0.9149,  0.7900,  0.6137,  0.6083,\n",
      "         -0.8105,  0.2053, -0.2882,  0.4052, -0.5155, -0.9344,  0.9315]],\n",
      "       grad_fn=<SliceBackward0>)}\n",
      "{'2-0-1': tensor([[39.4608]], grad_fn=<SliceBackward0>)}\n",
      "Counterfactual V1 evaluation\n",
      "0.45701651033214974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/utils/validation.py:746: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/utils/validation.py:746: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  array = np.asarray(array, order=order, dtype=dtype)\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_regression.py:96: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "v1_eval = ii_evaluation(convert_input(x_base_test_V1), \n",
    "                        convert_input(x_source_test_V1),\n",
    "                        y_base_test_V1, y_source_test_V1,\n",
    "                        model_regression, \"V1\", output_coord)\n",
    "print(\"Counterfactual V1 evaluation\")\n",
    "print(r2_score(y_base_test_V1, v1_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[46.]]),\n",
       " tensor([[14.]]),\n",
       " tensor([[14.]]),\n",
       " tensor([[7.]]),\n",
       " tensor([[45.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[44.]]),\n",
       " tensor([[28.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[20.]]),\n",
       " tensor([[40.]]),\n",
       " tensor([[0.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[43.]]),\n",
       " tensor([[10.]]),\n",
       " tensor([[32.]]),\n",
       " tensor([[43.]]),\n",
       " tensor([[29.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[32.]]),\n",
       " tensor([[17.]]),\n",
       " tensor([[40.]]),\n",
       " tensor([[44.]]),\n",
       " tensor([[24.]]),\n",
       " tensor([[26.]]),\n",
       " tensor([[15.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[27.]]),\n",
       " tensor([[26.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[13.]]),\n",
       " tensor([[36.]]),\n",
       " tensor([[19.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[20.]]),\n",
       " tensor([[19.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[13.]]),\n",
       " tensor([[26.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[7.]]),\n",
       " tensor([[42.]]),\n",
       " tensor([[45.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[32.]]),\n",
       " tensor([[30.]]),\n",
       " tensor([[16.]]),\n",
       " tensor([[25.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[45.]]),\n",
       " tensor([[34.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[13.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[12.]]),\n",
       " tensor([[18.]]),\n",
       " tensor([[29.]]),\n",
       " tensor([[42.]]),\n",
       " tensor([[15.]]),\n",
       " tensor([[41.]]),\n",
       " tensor([[25.]]),\n",
       " tensor([[45.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[6.]]),\n",
       " tensor([[28.]]),\n",
       " tensor([[45.]]),\n",
       " tensor([[24.]]),\n",
       " tensor([[40.]]),\n",
       " tensor([[11.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[10.]]),\n",
       " tensor([[34.]]),\n",
       " tensor([[18.]]),\n",
       " tensor([[17.]]),\n",
       " tensor([[12.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[10.]]),\n",
       " tensor([[33.]]),\n",
       " tensor([[31.]]),\n",
       " tensor([[28.]]),\n",
       " tensor([[36.]]),\n",
       " tensor([[15.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[4.]]),\n",
       " tensor([[21.]]),\n",
       " tensor([[41.]]),\n",
       " tensor([[25.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[9.]]),\n",
       " tensor([[13.]]),\n",
       " tensor([[24.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[3.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[46.]]),\n",
       " tensor([[22.]]),\n",
       " tensor([[39.]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIT on Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_coords = {\n",
    "    V1: [{\"layer\": 1, \"start\": 0, \"end\": embedding_dim*3}], \n",
    "    V2: [{\"layer\": 1, \"start\": embedding_dim*3, \"end\": embedding_dim*6}]    \n",
    "}\n",
    "\n",
    "both_regression_model = TorchLinearEmbeddingRegressionIIT(\n",
    "    hidden_dim=50, \n",
    "    hidden_activation=torch.nn.ReLU(), \n",
    "    num_layers=2, \n",
    "    id_to_coords=id_to_coords, \n",
    "    vocab=vals,\n",
    "    num_inputs=num_inputs,\n",
    "    embed_dim=5,\n",
    "    max_iter=2000,\n",
    "    batch_size=1028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 2000 of 2000; error is 94.658077239990231"
     ]
    }
   ],
   "source": [
    "_ = both_regression_model.fit(\n",
    "    X_base_train_both, \n",
    "    X_sources_train_both, \n",
    "    y_base_train_both, \n",
    "    y_source_train_both,\n",
    "    interventions_train_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 41.,  30., 110.,  24.,   0.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score \n",
    "\n",
    "IIT_preds_V1, base_preds_V1 = both_regression_model.iit_predict(\n",
    "    x_base_test_V1, [x_source_test_V1], interventions_test_V1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Evaluation\n",
      "0.9982170203239872\n",
      "V1 counterfactual evaluation\n",
      "0.9813394636650848\n"
     ]
    }
   ],
   "source": [
    "print('Standard Evaluation')\n",
    "print(r2_score(y_base_test_V1, base_preds_V1.detach()))\n",
    "      \n",
    "print(\"V1 counterfactual evaluation\")\n",
    "print(r2_score(y_source_test_V1, IIT_preds_V1.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "IIT_preds_V2, base_preds_V2 = both_regression_model.iit_predict(\n",
    "    x_base_test_V2, [x_source_test_V2], interventions_test_V2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2 counterfactual evaluation\n",
      "0.9941180023871107\n"
     ]
    }
   ],
   "source": [
    "print(\"V2 counterfactual evaluation\")\n",
    "print(r2_score(y_source_test_V2, IIT_preds_V2.detach()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23b0dade77de706664b673ec0b749cccbdb5cac5bb41527ce15ce25b9a737383"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
