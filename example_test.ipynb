{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from iit import generate_data, get_iit_distribution_dataset_both\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from model_classifier import TorchDeepNeuralEmbeddingClassifier\n",
    "from model_regression import TorchLinearEmbeddingRegression\n",
    "import model_classifier_iit\n",
    "from model_classifier_iit import TorchDeepNeuralClassifierIIT\n",
    "from model_regression_iit import TorchLinearEmbeddingRegressionIIT\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import vsm\n",
    "from sklearn.metrics import classification_report, r2_score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = ['zero', 'one', 'two', 'three', 'four',\n",
    "       'five', 'six', 'seven', 'eight', 'nine']\n",
    "\n",
    "train_test_split = 0.9\n",
    "X_train, y_train, X_test, y_test = generate_data(vals, train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['four', 'nine', 'four'],\n",
       " ['nine', 'nine', 'zero'],\n",
       " ['six', 'one', 'one'],\n",
       " ['seven', 'three', 'four'],\n",
       " ['five', 'nine', 'three']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 81, 12, 49, 60]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['eight', 'six', 'five'],\n",
       " ['six', 'nine', 'two'],\n",
       " ['two', 'one', 'zero'],\n",
       " ['nine', 'five', 'five'],\n",
       " ['two', 'five', 'five']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a feed-forward network using randomized embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 163\n",
    "num_inputs = 3\n",
    "num_layers = 2\n",
    "embed_dim = 5\n",
    "\n",
    "mod = TorchDeepNeuralEmbeddingClassifier(vals, output_size, num_inputs,\n",
    "                            num_layers, embed_dim, None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1000 of 1000; error is 0.10143007338047028"
     ]
    }
   ],
   "source": [
    "_ = mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94         8\n",
      "           1       1.00      0.00      0.00         1\n",
      "           2       1.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      1.00      0.00         0\n",
      "           5       0.00      1.00      0.00         0\n",
      "           6       1.00      1.00      1.00         1\n",
      "           7       1.00      0.75      0.86         4\n",
      "           8       1.00      0.67      0.80         3\n",
      "           9       0.00      1.00      0.00         0\n",
      "          12       0.50      1.00      0.67         1\n",
      "          13       1.00      1.00      1.00         1\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         1\n",
      "          16       1.00      0.75      0.86         4\n",
      "          17       0.00      1.00      0.00         0\n",
      "          18       1.00      0.50      0.67         2\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       1.00      1.00      1.00         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "          24       1.00      1.00      1.00         4\n",
      "          25       1.00      1.00      1.00         1\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      1.00      1.00         1\n",
      "          30       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         2\n",
      "          35       1.00      1.00      1.00         4\n",
      "          36       1.00      1.00      1.00         2\n",
      "          40       1.00      1.00      1.00         1\n",
      "          42       1.00      1.00      1.00         2\n",
      "          44       1.00      1.00      1.00         1\n",
      "          48       1.00      1.00      1.00         4\n",
      "          50       1.00      1.00      1.00         1\n",
      "          54       1.00      1.00      1.00         1\n",
      "          55       1.00      1.00      1.00         1\n",
      "          56       1.00      1.00      1.00         3\n",
      "          60       1.00      1.00      1.00         1\n",
      "          63       1.00      1.00      1.00         2\n",
      "          64       1.00      1.00      1.00         3\n",
      "          66       1.00      1.00      1.00         1\n",
      "          70       1.00      1.00      1.00         2\n",
      "          72       1.00      1.00      1.00         2\n",
      "          75       1.00      1.00      1.00         1\n",
      "          78       1.00      1.00      1.00         1\n",
      "          81       1.00      1.00      1.00         1\n",
      "          85       1.00      1.00      1.00         1\n",
      "          88       1.00      1.00      1.00         2\n",
      "          90       1.00      1.00      1.00         2\n",
      "          91       1.00      1.00      1.00         1\n",
      "         104       1.00      1.00      1.00         1\n",
      "         105       1.00      1.00      1.00         2\n",
      "         112       1.00      1.00      1.00         1\n",
      "         120       1.00      1.00      1.00         2\n",
      "         126       0.00      1.00      0.00         0\n",
      "         128       1.00      1.00      1.00         1\n",
      "         135       0.00      1.00      0.00         0\n",
      "         144       1.00      0.00      0.00         1\n",
      "         153       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.87      0.89      0.79       100\n",
      "weighted avg       0.96      0.89      0.90       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = mod.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, preds, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a feed-forward network using BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_weights_name = 'bert-base-uncased'\n",
    "# Initialize a BERT tokenizer and BERT model based on\n",
    "# `bert_weights_name`:\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "bert_model = BertModel.from_pretrained(bert_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embed = vsm.create_subword_pooling_vsm(\n",
    "    vals, bert_tokenizer, bert_model, layer=1, pool_func=vsm.mean_pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>0.368413</td>\n",
       "      <td>0.714821</td>\n",
       "      <td>-0.532399</td>\n",
       "      <td>-0.153238</td>\n",
       "      <td>-0.184203</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.028352</td>\n",
       "      <td>-0.162773</td>\n",
       "      <td>0.163669</td>\n",
       "      <td>-0.471809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637450</td>\n",
       "      <td>-0.782000</td>\n",
       "      <td>0.181008</td>\n",
       "      <td>-1.160265</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>-0.899586</td>\n",
       "      <td>0.350256</td>\n",
       "      <td>-0.099035</td>\n",
       "      <td>-0.274523</td>\n",
       "      <td>0.308869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.213226</td>\n",
       "      <td>0.484864</td>\n",
       "      <td>-0.032716</td>\n",
       "      <td>-0.026842</td>\n",
       "      <td>0.090642</td>\n",
       "      <td>-0.086201</td>\n",
       "      <td>0.195947</td>\n",
       "      <td>-0.169620</td>\n",
       "      <td>0.540456</td>\n",
       "      <td>-0.261407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455036</td>\n",
       "      <td>-0.426076</td>\n",
       "      <td>0.282586</td>\n",
       "      <td>-0.676856</td>\n",
       "      <td>-0.045337</td>\n",
       "      <td>-0.428130</td>\n",
       "      <td>0.227807</td>\n",
       "      <td>0.216206</td>\n",
       "      <td>0.112196</td>\n",
       "      <td>-0.128516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>-0.121083</td>\n",
       "      <td>0.161060</td>\n",
       "      <td>-0.549375</td>\n",
       "      <td>-0.472711</td>\n",
       "      <td>-0.146909</td>\n",
       "      <td>0.155344</td>\n",
       "      <td>-0.133190</td>\n",
       "      <td>-0.483241</td>\n",
       "      <td>0.173656</td>\n",
       "      <td>-0.328344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303860</td>\n",
       "      <td>0.224042</td>\n",
       "      <td>0.113168</td>\n",
       "      <td>-0.807975</td>\n",
       "      <td>-0.281597</td>\n",
       "      <td>-0.575383</td>\n",
       "      <td>0.138091</td>\n",
       "      <td>0.198803</td>\n",
       "      <td>-0.091571</td>\n",
       "      <td>-0.442494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>three</th>\n",
       "      <td>0.104971</td>\n",
       "      <td>0.313852</td>\n",
       "      <td>-0.340210</td>\n",
       "      <td>-0.434407</td>\n",
       "      <td>-0.049186</td>\n",
       "      <td>0.154321</td>\n",
       "      <td>-0.099751</td>\n",
       "      <td>-0.506876</td>\n",
       "      <td>0.389955</td>\n",
       "      <td>-0.244507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379173</td>\n",
       "      <td>0.134788</td>\n",
       "      <td>0.233690</td>\n",
       "      <td>-0.651362</td>\n",
       "      <td>-0.219017</td>\n",
       "      <td>-0.658988</td>\n",
       "      <td>0.203868</td>\n",
       "      <td>0.215337</td>\n",
       "      <td>-0.093137</td>\n",
       "      <td>-0.477617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>four</th>\n",
       "      <td>0.039704</td>\n",
       "      <td>0.210273</td>\n",
       "      <td>-0.526674</td>\n",
       "      <td>-0.221241</td>\n",
       "      <td>-0.102211</td>\n",
       "      <td>0.203096</td>\n",
       "      <td>-0.147871</td>\n",
       "      <td>-0.331833</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>-0.481170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240441</td>\n",
       "      <td>-0.040645</td>\n",
       "      <td>0.122894</td>\n",
       "      <td>-0.991067</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>-0.848153</td>\n",
       "      <td>0.368776</td>\n",
       "      <td>0.249214</td>\n",
       "      <td>-0.265129</td>\n",
       "      <td>-0.095733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>five</th>\n",
       "      <td>-0.039133</td>\n",
       "      <td>0.148628</td>\n",
       "      <td>-0.443105</td>\n",
       "      <td>-0.602812</td>\n",
       "      <td>0.024338</td>\n",
       "      <td>-0.077868</td>\n",
       "      <td>-0.239623</td>\n",
       "      <td>-0.467639</td>\n",
       "      <td>0.117604</td>\n",
       "      <td>-0.661229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410685</td>\n",
       "      <td>-0.126665</td>\n",
       "      <td>0.254384</td>\n",
       "      <td>-0.834398</td>\n",
       "      <td>-0.037767</td>\n",
       "      <td>-0.695847</td>\n",
       "      <td>0.188720</td>\n",
       "      <td>0.237780</td>\n",
       "      <td>-0.017827</td>\n",
       "      <td>-0.366614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>six</th>\n",
       "      <td>0.304316</td>\n",
       "      <td>0.202454</td>\n",
       "      <td>-0.506930</td>\n",
       "      <td>-0.336193</td>\n",
       "      <td>0.042346</td>\n",
       "      <td>-0.201237</td>\n",
       "      <td>-0.326339</td>\n",
       "      <td>-0.235801</td>\n",
       "      <td>0.244673</td>\n",
       "      <td>-0.600611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417288</td>\n",
       "      <td>-0.329488</td>\n",
       "      <td>0.127992</td>\n",
       "      <td>-0.932242</td>\n",
       "      <td>-0.041394</td>\n",
       "      <td>-0.668947</td>\n",
       "      <td>-0.093982</td>\n",
       "      <td>0.385106</td>\n",
       "      <td>0.048364</td>\n",
       "      <td>-0.338787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seven</th>\n",
       "      <td>0.092242</td>\n",
       "      <td>0.176112</td>\n",
       "      <td>-0.475537</td>\n",
       "      <td>-0.412275</td>\n",
       "      <td>0.071916</td>\n",
       "      <td>-0.310987</td>\n",
       "      <td>-0.048490</td>\n",
       "      <td>-0.409864</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>-0.807861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095583</td>\n",
       "      <td>-0.178779</td>\n",
       "      <td>0.421195</td>\n",
       "      <td>-0.693458</td>\n",
       "      <td>0.123755</td>\n",
       "      <td>-0.667253</td>\n",
       "      <td>0.076955</td>\n",
       "      <td>-0.138880</td>\n",
       "      <td>-0.079268</td>\n",
       "      <td>-0.244648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eight</th>\n",
       "      <td>-0.033224</td>\n",
       "      <td>0.192466</td>\n",
       "      <td>-0.507554</td>\n",
       "      <td>-0.419275</td>\n",
       "      <td>0.235861</td>\n",
       "      <td>0.105365</td>\n",
       "      <td>-0.255650</td>\n",
       "      <td>-0.200700</td>\n",
       "      <td>-0.053316</td>\n",
       "      <td>-0.733084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>-0.171980</td>\n",
       "      <td>0.286490</td>\n",
       "      <td>-0.620115</td>\n",
       "      <td>-0.104281</td>\n",
       "      <td>-0.584950</td>\n",
       "      <td>0.222393</td>\n",
       "      <td>0.159457</td>\n",
       "      <td>-0.355556</td>\n",
       "      <td>-0.570065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nine</th>\n",
       "      <td>0.105645</td>\n",
       "      <td>0.060501</td>\n",
       "      <td>-0.367164</td>\n",
       "      <td>-0.433360</td>\n",
       "      <td>0.425262</td>\n",
       "      <td>-0.032186</td>\n",
       "      <td>-0.136358</td>\n",
       "      <td>-0.358969</td>\n",
       "      <td>0.129830</td>\n",
       "      <td>-0.574778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081581</td>\n",
       "      <td>-0.280892</td>\n",
       "      <td>0.540993</td>\n",
       "      <td>-0.752147</td>\n",
       "      <td>-0.059858</td>\n",
       "      <td>-0.446215</td>\n",
       "      <td>0.033923</td>\n",
       "      <td>-0.036796</td>\n",
       "      <td>-0.296937</td>\n",
       "      <td>-0.314274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "zero   0.368413  0.714821 -0.532399 -0.153238 -0.184203  0.009934  0.028352   \n",
       "one    0.213226  0.484864 -0.032716 -0.026842  0.090642 -0.086201  0.195947   \n",
       "two   -0.121083  0.161060 -0.549375 -0.472711 -0.146909  0.155344 -0.133190   \n",
       "three  0.104971  0.313852 -0.340210 -0.434407 -0.049186  0.154321 -0.099751   \n",
       "four   0.039704  0.210273 -0.526674 -0.221241 -0.102211  0.203096 -0.147871   \n",
       "five  -0.039133  0.148628 -0.443105 -0.602812  0.024338 -0.077868 -0.239623   \n",
       "six    0.304316  0.202454 -0.506930 -0.336193  0.042346 -0.201237 -0.326339   \n",
       "seven  0.092242  0.176112 -0.475537 -0.412275  0.071916 -0.310987 -0.048490   \n",
       "eight -0.033224  0.192466 -0.507554 -0.419275  0.235861  0.105365 -0.255650   \n",
       "nine   0.105645  0.060501 -0.367164 -0.433360  0.425262 -0.032186 -0.136358   \n",
       "\n",
       "            7         8         9    ...       758       759       760  \\\n",
       "zero  -0.162773  0.163669 -0.471809  ...  0.637450 -0.782000  0.181008   \n",
       "one   -0.169620  0.540456 -0.261407  ...  0.455036 -0.426076  0.282586   \n",
       "two   -0.483241  0.173656 -0.328344  ...  0.303860  0.224042  0.113168   \n",
       "three -0.506876  0.389955 -0.244507  ...  0.379173  0.134788  0.233690   \n",
       "four  -0.331833  0.267574 -0.481170  ...  0.240441 -0.040645  0.122894   \n",
       "five  -0.467639  0.117604 -0.661229  ...  0.410685 -0.126665  0.254384   \n",
       "six   -0.235801  0.244673 -0.600611  ...  0.417288 -0.329488  0.127992   \n",
       "seven -0.409864  0.006404 -0.807861  ...  0.095583 -0.178779  0.421195   \n",
       "eight -0.200700 -0.053316 -0.733084  ...  0.233600 -0.171980  0.286490   \n",
       "nine  -0.358969  0.129830 -0.574778  ...  0.081581 -0.280892  0.540993   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "zero  -1.160265  0.005396 -0.899586  0.350256 -0.099035 -0.274523  0.308869  \n",
       "one   -0.676856 -0.045337 -0.428130  0.227807  0.216206  0.112196 -0.128516  \n",
       "two   -0.807975 -0.281597 -0.575383  0.138091  0.198803 -0.091571 -0.442494  \n",
       "three -0.651362 -0.219017 -0.658988  0.203868  0.215337 -0.093137 -0.477617  \n",
       "four  -0.991067  0.013572 -0.848153  0.368776  0.249214 -0.265129 -0.095733  \n",
       "five  -0.834398 -0.037767 -0.695847  0.188720  0.237780 -0.017827 -0.366614  \n",
       "six   -0.932242 -0.041394 -0.668947 -0.093982  0.385106  0.048364 -0.338787  \n",
       "seven -0.693458  0.123755 -0.667253  0.076955 -0.138880 -0.079268 -0.244648  \n",
       "eight -0.620115 -0.104281 -0.584950  0.222393  0.159457 -0.355556 -0.570065  \n",
       "nine  -0.752147 -0.059858 -0.446215  0.033923 -0.036796 -0.296937 -0.314274  \n",
       "\n",
       "[10 rows x 768 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 163\n",
    "num_inputs = 3\n",
    "num_layers = 2\n",
    "embed_dim = bert_embed.shape[1]\n",
    "freeze_embedding = True\n",
    "\n",
    "mod_bert_embed = TorchDeepNeuralEmbeddingClassifier(vals, output_size, num_inputs,\n",
    "                            num_layers, embed_dim, bert_embed,\n",
    "                            freeze_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1000 of 1000; error is 0.07325881719589233"
     ]
    }
   ],
   "source": [
    "_ = mod_bert_embed.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       1.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      1.00      0.00         0\n",
      "           5       0.00      1.00      0.00         0\n",
      "           6       0.50      1.00      0.67         1\n",
      "           7       1.00      0.50      0.67         4\n",
      "           8       1.00      0.33      0.50         3\n",
      "           9       0.00      1.00      0.00         0\n",
      "          12       0.25      1.00      0.40         1\n",
      "          13       1.00      0.00      0.00         1\n",
      "          14       0.00      0.00      0.00         3\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       1.00      0.75      0.86         4\n",
      "          18       0.50      0.50      0.50         2\n",
      "          20       1.00      1.00      1.00         3\n",
      "          21       0.50      1.00      0.67         1\n",
      "          22       1.00      1.00      1.00         1\n",
      "          24       1.00      0.75      0.86         4\n",
      "          25       0.50      1.00      0.67         1\n",
      "          27       0.50      0.33      0.40         3\n",
      "          28       1.00      1.00      1.00         1\n",
      "          30       0.67      1.00      0.80         2\n",
      "          32       1.00      1.00      1.00         2\n",
      "          35       1.00      0.75      0.86         4\n",
      "          36       1.00      0.50      0.67         2\n",
      "          40       0.50      1.00      0.67         1\n",
      "          42       0.67      1.00      0.80         2\n",
      "          44       1.00      1.00      1.00         1\n",
      "          48       1.00      1.00      1.00         4\n",
      "          50       1.00      1.00      1.00         1\n",
      "          54       1.00      1.00      1.00         1\n",
      "          55       1.00      1.00      1.00         1\n",
      "          56       0.75      1.00      0.86         3\n",
      "          60       0.50      1.00      0.67         1\n",
      "          63       1.00      1.00      1.00         2\n",
      "          64       1.00      0.67      0.80         3\n",
      "          66       1.00      0.00      0.00         1\n",
      "          70       1.00      1.00      1.00         2\n",
      "          72       1.00      1.00      1.00         2\n",
      "          75       1.00      1.00      1.00         1\n",
      "          78       1.00      1.00      1.00         1\n",
      "          81       1.00      1.00      1.00         1\n",
      "          85       1.00      1.00      1.00         1\n",
      "          88       1.00      0.50      0.67         2\n",
      "          90       0.67      1.00      0.80         2\n",
      "          91       1.00      1.00      1.00         1\n",
      "          96       0.00      1.00      0.00         0\n",
      "          98       0.00      1.00      0.00         0\n",
      "         104       1.00      1.00      1.00         1\n",
      "         105       1.00      0.50      0.67         2\n",
      "         112       0.33      1.00      0.50         1\n",
      "         120       1.00      0.00      0.00         2\n",
      "         128       1.00      0.00      0.00         1\n",
      "         135       0.00      1.00      0.00         0\n",
      "         136       0.00      1.00      0.00         0\n",
      "         144       1.00      0.00      0.00         1\n",
      "         153       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.70       100\n",
      "   macro avg       0.71      0.73      0.58       100\n",
      "weighted avg       0.83      0.70      0.70       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = mod_bert_embed.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, preds, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a BERT model by tokenizing inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HfBertClassifierModel(nn.Module):\n",
    "    def __init__(self, n_classes, weights_name='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.weights_name = weights_name\n",
    "        self.bert = BertModel.from_pretrained(self.weights_name)\n",
    "        self.bert.train()\n",
    "        self.hidden_dim = self.bert.embeddings.word_embeddings.embedding_dim\n",
    "        # The only new parameters -- the classifier:\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.hidden_dim, self.n_classes)\n",
    "\n",
    "    def forward(self, indices, mask):\n",
    "        reps = self.bert(\n",
    "            indices, attention_mask=mask)\n",
    "        return self.classifier_layer(reps.pooler_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HfBertClassifier(TorchShallowNeuralClassifier):\n",
    "    def __init__(self, weights_name, *args, **kwargs):\n",
    "        self.weights_name = weights_name\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.weights_name)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.params += ['weights_name']\n",
    "\n",
    "    def build_graph(self):\n",
    "        return HfBertClassifierModel(self.n_classes_, self.weights_name)\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        data = self.tokenizer.batch_encode_plus(\n",
    "            X,\n",
    "            max_length=None,\n",
    "            add_special_tokens=True,\n",
    "            padding='longest',\n",
    "            return_attention_mask=True)\n",
    "        indices = torch.tensor(data['input_ids'])\n",
    "        mask = torch.tensor(data['attention_mask'])\n",
    "        if y is None:\n",
    "            dataset = torch.utils.data.TensorDataset(indices, mask)\n",
    "        else:\n",
    "            self.classes_ = sorted(set(y))\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            y = [class2index[label] for label in y]\n",
    "            y = torch.tensor(y)\n",
    "            dataset = torch.utils.data.TensorDataset(indices, mask, y)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion(X):\n",
    "    new_X = []\n",
    "    for inpts in X:\n",
    "        new_X.append(' '.join(inpts))\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_bert = HfBertClassifier('bert-base-uncased', max_iter=5, batch_size=8, n_iter_no_change=5, early_stopping=True, hidden_dim=100, eta=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seven seven four',\n",
       " 'seven nine nine',\n",
       " 'four eight five',\n",
       " 'eight nine two',\n",
       " 'one five zero']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bert = conversion(X_train)\n",
    "X_test_bert = conversion(X_test)\n",
    "X_train_bert[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Finished epoch 5 of 5; error is 250.60060513019562"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HfBertClassifier(\n",
       "\tbatch_size=8,\n",
       "\tmax_iter=5,\n",
       "\teta=5e-05,\n",
       "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tgradient_accumulation_steps=1,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=True,\n",
       "\tn_iter_no_change=5,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05,\n",
       "\thidden_dim=100,\n",
       "\thidden_activation=Tanh(),\n",
       "\tweights_name=bert-base-uncased)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_bert.fit(X_train_bert, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93         8\n",
      "           1       0.00      0.00      0.00         1\n",
      "           4       0.50      0.50      0.50         2\n",
      "           6       0.50      1.00      0.67         1\n",
      "           8       0.00      0.00      0.00         5\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         2\n",
      "          12       0.50      0.20      0.29         5\n",
      "          15       0.00      0.00      0.00         1\n",
      "          17       0.00      0.00      0.00         2\n",
      "          18       0.07      1.00      0.13         1\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         3\n",
      "          24       0.25      0.60      0.35         5\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.00      0.00      0.00         2\n",
      "          28       0.00      0.00      0.00         4\n",
      "          30       0.00      0.00      0.00         6\n",
      "          32       0.00      0.00      0.00         2\n",
      "          36       0.29      1.00      0.44         2\n",
      "          40       0.50      1.00      0.67         3\n",
      "          42       1.00      0.50      0.67         4\n",
      "          44       0.00      0.00      0.00         2\n",
      "          45       0.00      0.00      0.00         1\n",
      "          48       0.00      0.00      0.00         3\n",
      "          49       0.00      0.00      0.00         0\n",
      "          51       0.00      0.00      0.00         1\n",
      "          54       0.00      0.00      0.00         1\n",
      "          55       0.00      0.00      0.00         2\n",
      "          56       0.00      0.00      0.00         3\n",
      "          60       0.00      0.00      0.00         2\n",
      "          63       0.33      1.00      0.50         1\n",
      "          64       0.00      0.00      0.00         1\n",
      "          70       0.00      0.00      0.00         1\n",
      "          72       0.56      0.83      0.67         6\n",
      "          77       0.00      0.00      0.00         1\n",
      "          78       0.00      0.00      0.00         1\n",
      "          81       0.00      0.00      0.00         2\n",
      "          84       0.00      0.00      0.00         2\n",
      "          90       0.20      1.00      0.33         1\n",
      "          91       0.00      0.00      0.00         0\n",
      "          96       1.00      1.00      1.00         1\n",
      "          98       0.00      0.00      0.00         2\n",
      "          99       0.00      0.00      0.00         1\n",
      "         112       0.00      0.00      0.00         3\n",
      "         126       0.00      0.00      0.00         0\n",
      "         135       0.00      0.00      0.00         1\n",
      "         144       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.29       100\n",
      "   macro avg       0.14      0.21      0.15       100\n",
      "weighted avg       0.24      0.29      0.24       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jamesflemings/anaconda3/envs/nlu/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "preds = mod_bert.predict(X_test_bert)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IIT Dataset that computes y + z then x * (x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = 1\n",
    "train_data_V1, test_data_V1 = get_iit_distribution_dataset_both(V1, vals, 0.9, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_base_train_V1, y_base_train_V1, x_source_train_V1, y_source_train_V1 = train_data_V1\n",
    "x_base_test_V1, y_base_test_V1, x_source_test_V1, y_source_test_V1 = test_data_V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['one', 'six', 'zero'], ['five', 'nine', 'zero'], ['seven', 'nine', 'six']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_base_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['five', 'two', 'four'],\n",
       " ['three', 'seven', 'six'],\n",
       " ['eight', 'eight', 'three']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_source_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 65, 77]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_source_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(x_base_train_V1)\n",
    "interventions_train_V1 = torch.zeros(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = len(x_base_test_V1)\n",
    "interventions_test_V1 = torch.zeros(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = 0\n",
    "coords = {\n",
    "    V1: [{\"layer\": 1, \"start\": 0, \"end\": 15}], \n",
    "}\n",
    "\n",
    "V1_model = TorchLinearEmbeddingRegressionIIT(\n",
    "    hidden_dim=50, \n",
    "    hidden_activation=torch.nn.ReLU(), \n",
    "    num_layers=3, \n",
    "    id_to_coords=coords, \n",
    "    embed_dim=5,\n",
    "    vocab=vals,\n",
    "    num_inputs=3,\n",
    "    max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 2000 of 2000; error is 5.2497859001159675"
     ]
    }
   ],
   "source": [
    "_ = V1_model.fit(\n",
    "    x_base_train_V1, \n",
    "    [x_source_train_V1], \n",
    "    y_base_train_V1, \n",
    "    y_source_train_V1, \n",
    "    interventions_train_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 85., 17., 58.,  0.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IIT_preds_V1, base_preds_V1 = V1_model.iit_predict(\n",
    "    x_base_test_V1, [x_source_test_V1], interventions_test_V1\n",
    ")\n",
    "#y_base_test_V1[:5]\n",
    "base_preds_V1.detach()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Evaluation\n",
      "0.9968243746800389\n",
      "V1 counterfactual evaluation\n",
      "0.9888175325568104\n"
     ]
    }
   ],
   "source": [
    "print('Standard Evaluation')\n",
    "print(r2_score(y_base_test_V1, base_preds_V1.detach()))\n",
    "      \n",
    "print(\"V1 counterfactual evaluation\")\n",
    "print(r2_score(y_source_test_V1, IIT_preds_V1.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IIT Dataset that computes xy and yz then xy + yz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = 1\n",
    "V2 = 2\n",
    "\n",
    "train_data_V1, test_data_V1 = get_iit_distribution_dataset_both(V1, vals)\n",
    "train_data_V2, test_data_V2 = get_iit_distribution_dataset_both(V2, vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_base_train_V1, y_base_train_V1, x_source_train_V1, y_source_train_V1 = train_data_V1\n",
    "x_base_test_V1, y_base_test_V1, x_source_test_V1, y_source_test_V1 = test_data_V1\n",
    "\n",
    "x_base_train_V2, y_base_train_V2, x_source_train_V2, y_source_train_V2 = train_data_V2\n",
    "x_base_test_V2, y_base_test_V2, x_source_test_V2, y_source_test_V2 = test_data_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(x_base_train_V1)\n",
    "interventions_train_V1 = torch.zeros(train_size)\n",
    "interventions_train_V2 = torch.ones(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = len(x_base_test_V1)\n",
    "interventions_test_V1 = torch.zeros(test_size)\n",
    "interventions_test_V2 = torch.ones(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nine', 'three', 'zero'], ['six', 'four', 'zero'], ['four', 'two', 'three']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_base_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['four', 'nine', 'zero'], ['nine', 'six', 'zero'], ['five', 'two', 'three']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_source_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 54, 22]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_source_train_V1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base_train_both = np.concatenate([np.array(x_base_train_V1),\n",
    "                                np.array(x_base_train_V2)], axis=0)\n",
    "\n",
    "X_sources_train_both = [np.concatenate([np.array(x_source_train_V1),\n",
    "                            np.array(x_source_train_V2)], axis=0)]\n",
    "\n",
    "y_base_train_both = np.concatenate([np.array(y_base_train_V1),\n",
    "                            np.array(y_base_train_V2)], axis=0)\n",
    "\n",
    "y_source_train_both = np.concatenate([np.array(y_source_train_V1),\n",
    "                            np.array(y_source_train_V2)])\n",
    "\n",
    "interventions_train_both = np.concatenate([np.array(interventions_train_V1),\n",
    "                            np.array(interventions_train_V2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base_test_both = np.concatenate([np.array(x_base_test_V1),\n",
    "                                np.array(x_base_test_V2)], axis=0)\n",
    "\n",
    "X_sources_test_both = [np.concatenate([np.array(x_source_test_V1),\n",
    "                            np.array(x_source_test_V2)], axis=0)]\n",
    "\n",
    "y_base_test_both = np.concatenate([np.array(y_base_test_V1),\n",
    "                            np.array(y_base_test_V2)], axis=0)\n",
    "\n",
    "y_source_test_both = np.concatenate([np.array(y_source_test_V1),\n",
    "                            np.array(y_source_test_V2)])\n",
    "\n",
    "interventions_test_both = np.concatenate([np.array(interventions_test_V1),\n",
    "                            np.array(interventions_test_V2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regression = TorchLinearEmbeddingRegression(vocab=vals, num_inputs=3,\n",
    "                                                 num_layers=2, hidden_dim=50,\n",
    "                                                 embed_dim=5, max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 2000 of 2000; error is 217.56869506835938"
     ]
    }
   ],
   "source": [
    "_ = model_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8118413078595739"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_regression.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8443045762604124"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 5\n",
    "\n",
    "id_to_coords = {\n",
    "    V1: [{\"layer\": 1, \"start\": 0, \"end\": embedding_dim*3}], \n",
    "    V2: [{\"layer\": 1, \"start\": embedding_dim*3, \"end\": embedding_dim*6}]    \n",
    "}\n",
    "\n",
    "both_regression_model = TorchLinearEmbeddingRegressionIIT(\n",
    "    hidden_dim=50, \n",
    "    hidden_activation=torch.nn.ReLU(), \n",
    "    num_layers=2, \n",
    "    id_to_coords=id_to_coords, \n",
    "    vocab=vals,\n",
    "    num_inputs=num_inputs,\n",
    "    embed_dim=5,\n",
    "    max_iter=2000,\n",
    "    batch_size=1028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 2000 of 2000; error is 94.658077239990231"
     ]
    }
   ],
   "source": [
    "_ = both_regression_model.fit(\n",
    "    X_base_train_both, \n",
    "    X_sources_train_both, \n",
    "    y_base_train_both, \n",
    "    y_source_train_both,\n",
    "    interventions_train_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 41.,  30., 110.,  24.,   0.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score \n",
    "\n",
    "IIT_preds_V1, base_preds_V1 = both_regression_model.iit_predict(\n",
    "    x_base_test_V1, [x_source_test_V1], interventions_test_V1\n",
    ")\n",
    "#y_base_test_V1[:5]\n",
    "base_preds_V1.detach()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Evaluation\n",
      "0.9982170203239872\n",
      "V1 counterfactual evaluation\n",
      "0.9813394636650848\n"
     ]
    }
   ],
   "source": [
    "print('Standard Evaluation')\n",
    "print(r2_score(y_base_test_V1, base_preds_V1.detach()))\n",
    "      \n",
    "print(\"V1 counterfactual evaluation\")\n",
    "print(r2_score(y_source_test_V1, IIT_preds_V1.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "IIT_preds_V2, base_preds_V2 = both_regression_model.iit_predict(\n",
    "    x_base_test_V2, [x_source_test_V2], interventions_test_V2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2 counterfactual evaluation\n",
      "0.9941180023871107\n"
     ]
    }
   ],
   "source": [
    "print(\"V2 counterfactual evaluation\")\n",
    "print(r2_score(y_source_test_V2, IIT_preds_V2.detach()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23b0dade77de706664b673ec0b749cccbdb5cac5bb41527ce15ce25b9a737383"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
